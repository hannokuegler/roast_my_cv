{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Quick CV Roaster - SIMPLE VERSION\n",
    "\n",
    "**Two Easy Options:**\n",
    "1. **Drag & Drop** your PDF into the `uploaded_cvs/` folder, then enter the filename below\n",
    "2. **Use existing CV** from the dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:24:55.222311Z",
     "start_time": "2025-11-23T11:24:55.219136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory\n",
    "\n",
    "from cv_processor import CVProcessor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\" Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Your API Key\n",
    "\n",
    "The API key is stored locally in a config.py file (not included in the project repository), for security reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:24:56.169842Z",
     "start_time": "2025-11-23T11:24:56.165440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded from config.py\n"
     ]
    }
   ],
   "source": [
    "# Load API key from config.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import GEMINI_API_KEY\n",
    "print(\"API key loaded from config.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:24:56.655506Z",
     "start_time": "2025-11-23T11:24:56.652099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CV Processor initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CV Processor with API key\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "processor = CVProcessor(api_key=GEMINI_API_KEY)\n",
    "print(\" CV Processor initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload Your PDF CV (Simple Method)\n",
    "\n",
    "**Instructions:**\n",
    "1. Create folder `uploaded_cvs/` in the notebooks directory (if it doesn't exist)\n",
    "2. Drag and drop your PDF into that folder\n",
    "3. Enter the filename below and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:25:16.338303Z",
     "start_time": "2025-11-23T11:25:16.335065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Upload folder: /Users/hannokuegler/Library/CloudStorage/OneDrive-WUWien/SBWL/Data Science/4_LLM/roast_my_cv/roast_my_cv/notebooks/uploaded_cvs\n",
      "\n",
      "1. Copy your PDF CV into this folder\n",
      "2. Then enter the filename below and run the next cell\n"
     ]
    }
   ],
   "source": [
    "# Create upload directory if it doesn't exist\n",
    "upload_dir = Path('uploaded_cvs')\n",
    "upload_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\" Upload folder: {upload_dir.absolute()}\")\n",
    "print(\"\\n1. Copy your PDF CV into this folder\")\n",
    "print(\"2. Then enter the filename below and run the next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:25:17.146472Z",
     "start_time": "2025-11-23T11:25:16.972799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found PDF: uploaded_cvs/hannokuegler.pdf\n",
      " Extracted 1792 characters\n",
      "\n",
      "Preview:\n",
      "--------------------------------------------------------------------------------\n",
      "Education ——————————\n",
      "10/2023 – Wirtschaftsuniversität Wien\n",
      "today Welthandelsplatz 1 1020 Wien (5rd Semester)\n",
      "§ economic and social sciences\n",
      "§ business informatics\n",
      "§ specialization: data science, production\n",
      "and operations management\n",
      "§ top 1% overall\n",
      "09/2017 – HTL Mössingerstraße\n",
      "06/2022 Mössingerstraße 25, 9020 Klagenfurt\n",
      "§ electronics and technical informatics.\n",
      "§ final grade (Matura): 1,6\n",
      "Hanno horst Kügler\n",
      "§ programming, network engineering\n",
      "§ creativity in developing new solutions\n",
      "§ analytical \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ENTER YOUR PDF FILENAME HERE:\n",
    "# ==========================================\n",
    "PDF_FILENAME = \"hannokuegler.pdf\"  # Change this to your PDF filename\n",
    "# ==========================================\n",
    "\n",
    "pdf_path = upload_dir / PDF_FILENAME\n",
    "\n",
    "if pdf_path.exists():\n",
    "    print(f\" Found PDF: {pdf_path}\")\n",
    "    \n",
    "    # Extract text\n",
    "    cv_text = processor.extract_text_from_pdf(str(pdf_path))\n",
    "    \n",
    "    print(f\" Extracted {len(cv_text)} characters\\n\")\n",
    "    print(f\"Preview:\\n{'-'*80}\")\n",
    "    print(cv_text[:500])\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    use_uploaded = True\n",
    "    \n",
    "else:\n",
    "    print(f\" PDF not found: {pdf_path}\")\n",
    "    print(f\"\\nMake sure:\")\n",
    "    print(f\"1. PDF is in folder: {upload_dir.absolute()}\")\n",
    "    print(f\"2. Filename matches: {PDF_FILENAME}\")\n",
    "    print(f\"\\nOr use Option 3B to select from dataset instead.\")\n",
    "    \n",
    "    cv_text = None\n",
    "    use_uploaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:25:17.827191Z",
     "start_time": "2025-11-23T11:25:17.652778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset has 9544 CVs\n",
      "\n",
      "First 10 CVs:\n",
      "  0: Big data analytics working and database warehouse manager wi...\n",
      "  1: Fresher looking to join as a data analyst and junior data sc...\n",
      "  2: nan\n",
      "  3: To obtain a position in a fast-paced business office environ...\n",
      "  4: Professional accountant with an outstanding work ethic and i...\n",
      "  5: To secure an IT specialist, desktop support, network adminis...\n",
      "  6: nan\n",
      "  7: nan\n",
      "  8: Certified Data analyst with a degree in Electronics Engineer...\n",
      "  9: nan\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/resume_data.csv')\n",
    "\n",
    "print(f\" Dataset has {len(df)} CVs\")\n",
    "print(\"\\nFirst 10 CVs:\")\n",
    "for i in range(min(10, len(df))):\n",
    "    obj = df.iloc[i].get('career_objective', 'No objective')\n",
    "    preview = str(obj)[:60] + \"...\" if pd.notna(obj) and len(str(obj)) > 60 else str(obj)\n",
    "    print(f\"  {i}: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:25:18.470733Z",
     "start_time": "2025-11-23T11:25:18.464020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ Using uploaded PDF - skip this cell or set use_uploaded=False above to use dataset instead\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SELECT CV INDEX (0-9):\n",
    "# ==========================================\n",
    "CV_INDEX = 0  # Change this number (0-9)\n",
    "# ==========================================\n",
    "\n",
    "if not use_uploaded or cv_text is None:\n",
    "    # Format CV from dataset\n",
    "    def format_cv_for_llm(resume_row):\n",
    "        cv_text = []\n",
    "        \n",
    "        if pd.notna(resume_row.get('career_objective')):\n",
    "            cv_text.append(f\"CAREER OBJECTIVE:\\n{resume_row['career_objective']}\")\n",
    "        \n",
    "        if pd.notna(resume_row.get('skills')):\n",
    "            cv_text.append(f\"\\nSKILLS:\\n{resume_row['skills']}\")\n",
    "        \n",
    "        education_parts = []\n",
    "        if pd.notna(resume_row.get('educational_institution_name')):\n",
    "            education_parts.append(f\"Institution: {resume_row['educational_institution_name']}\")\n",
    "        if pd.notna(resume_row.get('degree_names')):\n",
    "            education_parts.append(f\"Degree: {resume_row['degree_names']}\")\n",
    "        if pd.notna(resume_row.get('major_field_of_studies')):\n",
    "            education_parts.append(f\"Major: {resume_row['major_field_of_studies']}\")\n",
    "        \n",
    "        if education_parts:\n",
    "            cv_text.append(f\"\\nEDUCATION:\\n\" + \"\\n\".join(education_parts))\n",
    "        \n",
    "        work_parts = []\n",
    "        if pd.notna(resume_row.get('professional_company_names')):\n",
    "            work_parts.append(f\"Company: {resume_row['professional_company_names']}\")\n",
    "        if pd.notna(resume_row.get('positions')):\n",
    "            work_parts.append(f\"Position: {resume_row['positions']}\")\n",
    "        if pd.notna(resume_row.get('responsibilities')):\n",
    "            work_parts.append(f\"Responsibilities:\\n{resume_row['responsibilities']}\")\n",
    "        \n",
    "        if work_parts:\n",
    "            cv_text.append(f\"\\nWORK EXPERIENCE:\\n\" + \"\\n\".join(work_parts))\n",
    "        \n",
    "        return \"\\n\".join(cv_text)\n",
    "    \n",
    "    cv_text = format_cv_for_llm(df.iloc[CV_INDEX])\n",
    "    \n",
    "    print(f\" Loaded CV #{CV_INDEX} from dataset\\n\")\n",
    "    print(f\"Preview:\\n{'-'*80}\")\n",
    "    print(cv_text[:500])\n",
    "    print(f\"{'-'*80}\")\n",
    "else:\n",
    "    print(\"ℹ Using uploaded PDF - skip this cell or set use_uploaded=False above to use dataset instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Choose Roaster Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:25:23.023089Z",
     "start_time": "2025-11-23T11:25:23.017960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Selected model: BRUTAL\n",
      " Brutal - Savage & funny (Temperature: 0.9)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CHOOSE MODEL: 'gentle', 'medium', 'brutal', or 'all'\n",
    "# ==========================================\n",
    "ROASTER_MODEL = 'brutal'  # Change to: 'gentle', 'medium', 'brutal', or 'all'\n",
    "# ==========================================\n",
    "\n",
    "print(f\" Selected model: {ROASTER_MODEL.upper()}\")\n",
    "\n",
    "if ROASTER_MODEL == 'gentle':\n",
    "    print(\" Gentle - Constructive & encouraging (Temperature: 0.4)\")\n",
    "elif ROASTER_MODEL == 'medium':\n",
    "    print(\" Medium - Direct & honest (Temperature: 0.7)\")\n",
    "elif ROASTER_MODEL == 'brutal':\n",
    "    print(\" Brutal - Savage & funny (Temperature: 0.9)\")\n",
    "elif ROASTER_MODEL == 'all':\n",
    "    print(\" All Three - Side-by-side comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Roast! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:25:45.562389Z",
     "start_time": "2025-11-23T11:25:24.319192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating critique(s)...\n",
      "\n",
      "Generating gentle critique...\n",
      "Generating medium critique...\n",
      "Generating brutal critique...\n",
      "================================================================================\n",
      " BRUTAL ROASTER CRITIQUE\n",
      "================================================================================\n",
      "Alright, buckle up, buttercup. We're about to dissect this CV like a frog in a high school biology class, except this time, the frog can talk back (and hopefully learn something).\n",
      "\n",
      "**OPENING ROAST:**\n",
      "\n",
      "\"Hanno horst Kügler\"... Okay, first things first, are you sure you're not a character from a Wes Anderson film? That name is so exquisitely Austrian, it's practically yodeling at me. It's got that certain \"I own a ski chalet and judge competitive strudel baking\" vibe.\n",
      "\n",
      "**CAREER OBJECTIVE AUTOPSY:**\n",
      "\n",
      "Where is it? Oh that's right, you're one of those \"I'm too cool for a career objective\" types. Instead, you're hoping employers will just *intuit* your deepest professional desires from your scattershot list of skills. I'd bet my next paycheck you're hoping to use AI and Traffic analysis to take over the world. Good Luck!\n",
      "\n",
      "**SKILLS COMEDY:**\n",
      "\n",
      "\"Creativity in developing new solutions\" – said everyone, ever. What new solution did you create? Did you invent a self-folding laundry basket? A cat that doesn't shed? Hit me with details, because you're currently as creative as a beige wall.\n",
      "\n",
      "\"Analytical and logical thinking\" – Congratulations, you're a human being... allegedly.\n",
      "\n",
      "And \"Spanish: A1\"? That's not fluent, my friend, that's barely conversational. That's the level where you can order a cerveza and ask where the bathroom is. Good enough to get you in trouble, not good enough to get you a job.\n",
      "\n",
      "**EXPERIENCE CHECK:**\n",
      "\n",
      "\"IT-Assistant, WU Wien, part-time, event management, media technology troubleshooting.\" So, you're the guy who plugs in the projector and makes sure the WiFi works? You are the hero we didn't know we needed! But for the love of Mozart, what's the impact? Did you increase event attendance by 400% by fixing the glitchy microphone? Did you single-handedly prevent a PowerPoint presentation from crashing during the Dean's speech? Give me some sizzle!\n",
      "\n",
      "\"Emergency medical services, Österreichisches Rotes Kreuz.\" Okay, this is actually impressive. Saving lives is cool. But are you telling me you can go from sticking needles in arms to troubleshooting printers? And somehow you think that *isn't* whiplash-inducing for a potential employer?\n",
      "\n",
      "**FATAL FLAWS:**\n",
      "\n",
      "*   **Formatting Follies:** The random bolding, the inconsistent bullet points, the wall of text... this CV looks like it was assembled by a caffeinated squirrel.\n",
      "*   **Lack of Quantifiable Results:** You list responsibilities, but no accomplishments. Where are the numbers, Hanno? Did you reduce IT costs by X percent? Did you increase traffic analysis accuracy by Y percent? Make me believe you're not just another cog in the machine.\n",
      "*   **The \"Everything and the Kitchen Sink\" Approach:** You've crammed in every skill and certificate you've ever acquired, even the ones that aren't relevant. Less is more, Hanno. Focus on the skills that align with the job you're applying for.\n",
      "\n",
      "**MIC DROP:**\n",
      "\n",
      "Hanno, you're clearly a bright guy. You've got a solid education and some interesting experiences. But this CV reads like a laundry list of vaguely impressive achievements. It needs a serious makeover. Polish it up, quantify your accomplishments, and for God's sake, pick a font and stick with it. Otherwise, you'll be stuck troubleshooting printers at WU Wien forever. And with a name like Hanno Horst Kügler, that would be a tragedy. I expect to see a revised version on my desk by Monday morning. Don't disappoint me.\n",
      "\n",
      "\n",
      "\n",
      " Critique(s) generated!\n"
     ]
    }
   ],
   "source": [
    "if cv_text is None:\n",
    "    print(\" No CV loaded! Go back to Step 3A or 3B\")\n",
    "else:\n",
    "    print(\" Generating critique(s)...\\n\")\n",
    "    \n",
    "    # Generate critiques\n",
    "    critiques = processor.generate_critiques(cv_text)\n",
    "    \n",
    "    # Display based on selection\n",
    "    if ROASTER_MODEL == 'all':\n",
    "        for model_name in ['gentle', 'medium', 'brutal']:\n",
    "            icon = {'gentle': '', 'medium': '', 'brutal': ''}[model_name]\n",
    "            print(\"=\"*80)\n",
    "            print(f\"{icon} {model_name.upper()} ROASTER\")\n",
    "            print(\"=\"*80)\n",
    "            print(critiques[model_name])\n",
    "            print(\"\\n\" * 2)\n",
    "    else:\n",
    "        icon = {'gentle': '', 'medium': '', 'brutal': ''}[ROASTER_MODEL]\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{icon} {ROASTER_MODEL.upper()} ROASTER CRITIQUE\")\n",
    "        print(\"=\"*80)\n",
    "        print(critiques[ROASTER_MODEL])\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\" Critique(s) generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation Metrics \n",
    "\n",
    "**Precision, Recall, and F1 Scores:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T11:13:29.777015Z",
     "start_time": "2025-11-23T11:13:29.747606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CALCULATING METRICS...\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - ALL MODELS\n",
      "================================================================================\n",
      "\n",
      " Performance Metrics:\n",
      "\n",
      " model  precision  recall  f1_score  coverage_rate\n",
      "gentle   0.333333     1.0  0.500000              0\n",
      "medium   0.500000     1.0  0.666667              0\n",
      "brutal   0.333333     0.5  0.400000              0\n",
      "\n",
      " Best Model: MEDIUM (F1: 66.67%)\n",
      "\n",
      "================================================================================\n",
      "DETAILED ANALYSIS - BRUTAL MODEL\n",
      "================================================================================\n",
      "\n",
      " Issue Detection Quality:\n",
      "   Precision:  33.33%  (How accurate are the critique's claims?)\n",
      "   Recall:     50.00%  (How many real issues were caught?)\n",
      "   F1 Score:   40.00%  (Overall balance)\n",
      "\n",
      " Confusion Matrix:\n",
      "   True Positives:  1   (Correctly identified issues)\n",
      "   False Positives: 2    (False alarms)\n",
      "   False Negatives: 1   (Missed issues)\n",
      "\n",
      " Issue Breakdown:\n",
      "   Actual CV Issues (ground truth):  ['no_metrics', 'formatting']\n",
      "   Issues mentioned in critique:     ['typos', 'vague_objective', 'formatting']\n",
      "     Issues MISSED by critique:     ['no_metrics']\n",
      "   ℹ  Extra issues mentioned:         ['typos', 'vague_objective']\n",
      "\n",
      " Section Coverage:\n",
      "   Coverage Rate: 0.00%\n",
      "   Sections Addressed: 0/0\n",
      "\n",
      "================================================================================\n",
      "\n",
      " INTERPRETATION:\n",
      "    POOR: Low quality critique (F1 < 0.5)\n",
      "     Low precision: Critique may be making up issues\n",
      "     Low recall: Critique is missing important issues\n"
     ]
    }
   ],
   "source": [
    "if cv_text and 'critiques' in locals():\n",
    "    print(\" CALCULATING METRICS...\\n\")\n",
    "    \n",
    "    # Evaluate all models\n",
    "    df_results = processor.evaluate_all_models(cv_text, critiques)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUATION RESULTS - ALL MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n Performance Metrics:\\n\")\n",
    "    print(df_results[['model', 'precision', 'recall', 'f1_score', 'coverage_rate']].to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_idx = df_results['f1_score'].idxmax()\n",
    "    best_model = df_results.loc[best_idx, 'model']\n",
    "    best_f1 = df_results.loc[best_idx, 'f1_score']\n",
    "    \n",
    "    print(f\"\\n Best Model: {best_model.upper()} (F1: {best_f1:.2%})\")\n",
    "    \n",
    "    # Detailed analysis for selected or best model\n",
    "    if ROASTER_MODEL == 'all':\n",
    "        analysis_model = best_model\n",
    "    else:\n",
    "        analysis_model = ROASTER_MODEL\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED ANALYSIS - {analysis_model.upper()} MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    detection = processor.calculate_issue_detection_metrics(cv_text, critiques[analysis_model])\n",
    "    coverage = processor.calculate_section_coverage(cv_text, critiques[analysis_model])\n",
    "    \n",
    "    print(f\"\\n Issue Detection Quality:\")\n",
    "    print(f\"   Precision:  {detection['precision']:.2%}  (How accurate are the critique's claims?)\")\n",
    "    print(f\"   Recall:     {detection['recall']:.2%}  (How many real issues were caught?)\")\n",
    "    print(f\"   F1 Score:   {detection['f1_score']:.2%}  (Overall balance)\")\n",
    "    \n",
    "    print(f\"\\n Confusion Matrix:\")\n",
    "    print(f\"   True Positives:  {detection['true_positives']}   (Correctly identified issues)\")\n",
    "    print(f\"   False Positives: {detection['false_positives']}    (False alarms)\")\n",
    "    print(f\"   False Negatives: {detection['false_negatives']}   (Missed issues)\")\n",
    "    \n",
    "    print(f\"\\n Issue Breakdown:\")\n",
    "    print(f\"   Actual CV Issues (ground truth):  {detection['ground_truth_issues']}\")\n",
    "    print(f\"   Issues mentioned in critique:     {detection['detected_issues']}\")\n",
    "    \n",
    "    if detection['missed_issues']:\n",
    "        print(f\"     Issues MISSED by critique:     {detection['missed_issues']}\")\n",
    "    \n",
    "    if detection['extra_mentions']:\n",
    "        print(f\"   ℹ  Extra issues mentioned:         {detection['extra_mentions']}\")\n",
    "    \n",
    "    print(f\"\\n Section Coverage:\")\n",
    "    print(f\"   Coverage Rate: {coverage['coverage_rate']:.2%}\")\n",
    "    print(f\"   Sections Addressed: {coverage['sections_addressed_in_critique']}/{coverage['total_sections_in_cv']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n INTERPRETATION:\")\n",
    "    if detection['f1_score'] >= 0.7:\n",
    "        print(\"    GOOD: This critique has high quality (F1 ≥ 0.7)\")\n",
    "    elif detection['f1_score'] >= 0.5:\n",
    "        print(\"     ACCEPTABLE: Decent quality but could be better (F1 = 0.5-0.7)\")\n",
    "    else:\n",
    "        print(\"    POOR: Low quality critique (F1 < 0.5)\")\n",
    "    \n",
    "    if detection['precision'] < 0.6:\n",
    "        print(\"     Low precision: Critique may be making up issues\")\n",
    "    if detection['recall'] < 0.6:\n",
    "        print(\"     Low recall: Critique is missing important issues\")\n",
    "    if coverage['coverage_rate'] >= 0.8:\n",
    "        print(\"    Excellent coverage: Most CV sections were reviewed\")\n",
    "    \n",
    "else:\n",
    "    print(\" Run Step 5 first to generate critiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Quick Summary\n",
    "\n",
    "### How to Use This Notebook:\n",
    "\n",
    "1. **Setup** (Step 1-2): Import libraries and add API key\n",
    "2. **Input CV** (Step 3):\n",
    "   - **Option A**: Put PDF in `uploaded_cvs/` folder, enter filename\n",
    "   - **Option B**: Load existing CV from dataset\n",
    "3. **Choose Model** (Step 4): Set `ROASTER_MODEL` variable\n",
    "4. **Generate** (Step 5): Get your roast!\n",
    "5. **Metrics** (Step 6): See precision, recall, F1 scores\n",
    "\n",
    "\n",
    "### Understanding Metrics:\n",
    "\n",
    "| Metric | What it means | Good Score |\n",
    "|--------|---------------|------------|\n",
    "| **Precision** | % of mentioned issues that are real | > 0.7 |\n",
    "| **Recall** | % of real issues that were caught | > 0.7 |\n",
    "| **F1 Score** | Overall quality (balance of both) | > 0.7 |\n",
    "| **Coverage** | % of CV sections reviewed | > 0.8 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
