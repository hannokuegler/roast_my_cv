{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Quick CV Roaster - SIMPLE VERSION\n",
    "\n",
    "**Two Easy Options:**\n",
    "1. **Drag & Drop** your PDF into the `uploaded_cvs/` folder, then enter the filename below\n",
    "2. **Use existing CV** from the dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:24:15.879393Z",
     "start_time": "2025-11-23T23:24:15.874684Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory\n",
    "\n",
    "from cv_processor import CVProcessor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\"Imports successful!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Your API Key\n",
    "\n",
    "The API key is stored locally in a config.py file (not included in the project repository), for security reasons"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:24:20.290420Z",
     "start_time": "2025-11-23T23:24:20.286656Z"
    }
   },
   "source": [
    "# Load API key from config.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import GEMINI_API_KEY\n",
    "print(\"API key loaded from config.py\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded from config.py\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:24:27.618877Z",
     "start_time": "2025-11-23T23:24:27.614625Z"
    }
   },
   "source": [
    "# Initialize the CV Processor with API key\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "processor = CVProcessor(api_key=GEMINI_API_KEY)\n",
    "print(\" CV Processor initialized successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CV Processor initialized successfully!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload Your PDF CV (Simple Method)\n",
    "\n",
    "**Instructions:**\n",
    "1. Create folder `uploaded_cvs/` in the notebooks directory (if it doesn't exist)\n",
    "2. Drag and drop your PDF into that folder\n",
    "3. Enter the filename below and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:24:31.417844Z",
     "start_time": "2025-11-23T23:24:31.413961Z"
    }
   },
   "source": [
    "# Create upload directory if it doesn't exist\n",
    "upload_dir = Path('uploaded_cvs')\n",
    "upload_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\" Upload folder: {upload_dir.absolute()}\")\n",
    "print(\"\\n1. Copy your PDF CV into this folder\")\n",
    "print(\"2. Then enter the filename below and run the next cell\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Upload folder: /Users/hannokuegler/Library/CloudStorage/OneDrive-WUWien/SBWL/Data Science/4_LLM/roast_my_cv/roast_my_cv/notebooks/uploaded_cvs\n",
      "\n",
      "1. Copy your PDF CV into this folder\n",
      "2. Then enter the filename below and run the next cell\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:24:43.116945Z",
     "start_time": "2025-11-23T23:24:42.959660Z"
    }
   },
   "source": [
    "# ==========================================\n",
    "# ENTER YOUR PDF FILENAME HERE:\n",
    "# ==========================================\n",
    "PDF_FILENAME = \"raficv.pdf\"  # Change this to your PDF filename\n",
    "# ==========================================\n",
    "\n",
    "pdf_path = upload_dir / PDF_FILENAME\n",
    "\n",
    "if pdf_path.exists():\n",
    "    print(f\" Found PDF: {pdf_path}\")\n",
    "    \n",
    "    # Extract text\n",
    "    cv_text = processor.extract_text_from_pdf(str(pdf_path))\n",
    "    \n",
    "    print(f\" Extracted {len(cv_text)} characters\\n\")\n",
    "    print(f\"Preview:\\n{'-'*80}\")\n",
    "    print(cv_text[:500])\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    use_uploaded = True\n",
    "    \n",
    "else:\n",
    "    print(f\" PDF not found: {pdf_path}\")\n",
    "    print(f\"\\nMake sure:\")\n",
    "    print(f\"1. PDF is in folder: {upload_dir.absolute()}\")\n",
    "    print(f\"2. Filename matches: {PDF_FILENAME}\")\n",
    "    print(f\"\\nOr use Option 3B to select from dataset instead.\")\n",
    "    \n",
    "    cv_text = None\n",
    "    use_uploaded = False"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found PDF: uploaded_cvs/raficv.pdf\n",
      " Extracted 1354 characters\n",
      "\n",
      "Preview:\n",
      "--------------------------------------------------------------------------------\n",
      "R A F A E L K A C S I C S\n",
      "B E R U F S E R F A H R U N G\n",
      "04/2023 – 09/2023\n",
      "Junior-Trader (Edelmetallhandel), GVS Austria, Wien\n",
      "Betreuung internationaler Geschäftskunden\n",
      "Beratung bei individuellen Kundenanfragen\n",
      "Administration & Koordination interner Abläufe\n",
      "Leithastraße 22/1/14, 1200 Wien\n",
      "Geboren am 07. November 2000\n",
      "10/2022 – 03/2023\n",
      "Servicekraft, Ignaz & Rosalia Marktcafé, Wien\n",
      "K O N T A K T Freundlicher Gästeservice & Beratung\n",
      "Kassiertätigkeiten & Lagerorganisation\n",
      "+43 650 2825178\n",
      "rafael00.kac\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:25:03.394035Z",
     "start_time": "2025-11-23T23:25:03.386017Z"
    }
   },
   "source": [
    "# ==========================================\n",
    "# SELECT CV INDEX (0-9):\n",
    "# ==========================================\n",
    "CV_INDEX = 0  # Change this number (0-9)\n",
    "# ==========================================\n",
    "\n",
    "if not use_uploaded or cv_text is None:\n",
    "    # Format CV from dataset\n",
    "    def format_cv_for_llm(resume_row):\n",
    "        cv_text = []\n",
    "        \n",
    "        if pd.notna(resume_row.get('career_objective')):\n",
    "            cv_text.append(f\"CAREER OBJECTIVE:\\n{resume_row['career_objective']}\")\n",
    "        \n",
    "        if pd.notna(resume_row.get('skills')):\n",
    "            cv_text.append(f\"\\nSKILLS:\\n{resume_row['skills']}\")\n",
    "        \n",
    "        education_parts = []\n",
    "        if pd.notna(resume_row.get('educational_institution_name')):\n",
    "            education_parts.append(f\"Institution: {resume_row['educational_institution_name']}\")\n",
    "        if pd.notna(resume_row.get('degree_names')):\n",
    "            education_parts.append(f\"Degree: {resume_row['degree_names']}\")\n",
    "        if pd.notna(resume_row.get('major_field_of_studies')):\n",
    "            education_parts.append(f\"Major: {resume_row['major_field_of_studies']}\")\n",
    "        \n",
    "        if education_parts:\n",
    "            cv_text.append(f\"\\nEDUCATION:\\n\" + \"\\n\".join(education_parts))\n",
    "        \n",
    "        work_parts = []\n",
    "        if pd.notna(resume_row.get('professional_company_names')):\n",
    "            work_parts.append(f\"Company: {resume_row['professional_company_names']}\")\n",
    "        if pd.notna(resume_row.get('positions')):\n",
    "            work_parts.append(f\"Position: {resume_row['positions']}\")\n",
    "        if pd.notna(resume_row.get('responsibilities')):\n",
    "            work_parts.append(f\"Responsibilities:\\n{resume_row['responsibilities']}\")\n",
    "        \n",
    "        if work_parts:\n",
    "            cv_text.append(f\"\\nWORK EXPERIENCE:\\n\" + \"\\n\".join(work_parts))\n",
    "        \n",
    "        return \"\\n\".join(cv_text)\n",
    "    \n",
    "    cv_text = format_cv_for_llm(df.iloc[CV_INDEX])\n",
    "    \n",
    "    print(f\" Loaded CV #{CV_INDEX} from dataset\\n\")\n",
    "    print(f\"Preview:\\n{'-'*80}\")\n",
    "    print(cv_text[:500])\n",
    "    print(f\"{'-'*80}\")\n",
    "else:\n",
    "    print(\"ℹ Using uploaded PDF - skip this cell or set use_uploaded=False above to use dataset instead\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ Using uploaded PDF - skip this cell or set use_uploaded=False above to use dataset instead\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Choose Roaster Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:25:15.317289Z",
     "start_time": "2025-11-23T23:25:15.311241Z"
    }
   },
   "source": [
    "# ==========================================\n",
    "# CHOOSE MODEL: 'gentle', 'medium', 'brutal', or 'all'\n",
    "# ==========================================\n",
    "ROASTER_MODEL = 'brutal'  # Change to: 'gentle', 'medium', 'brutal', or 'all'\n",
    "# ==========================================\n",
    "\n",
    "print(f\" Selected model: {ROASTER_MODEL.upper()}\")\n",
    "\n",
    "if ROASTER_MODEL == 'gentle':\n",
    "    print(\" Gentle - Constructive & encouraging (Temperature: 0.4)\")\n",
    "elif ROASTER_MODEL == 'medium':\n",
    "    print(\" Medium - Direct & honest (Temperature: 0.7)\")\n",
    "elif ROASTER_MODEL == 'brutal':\n",
    "    print(\" Brutal - Savage & funny (Temperature: 0.9)\")\n",
    "elif ROASTER_MODEL == 'all':\n",
    "    print(\" All Three - Side-by-side comparison\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Selected model: BRUTAL\n",
      " Brutal - Savage & funny (Temperature: 0.9)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Roast! "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:25:39.917982Z",
     "start_time": "2025-11-23T23:25:22.628973Z"
    }
   },
   "source": [
    "if cv_text is None:\n",
    "    print(\" No CV loaded!\")\n",
    "else:\n",
    "    print(\" Generating critique(s)...\\n\")\n",
    "    \n",
    "    # Generate critiques\n",
    "    critiques = processor.generate_critiques(cv_text)\n",
    "    \n",
    "    # Display based on selection\n",
    "    if ROASTER_MODEL == 'all':\n",
    "        for model_name in ['gentle', 'medium', 'brutal']:\n",
    "            icon = {'gentle': '', 'medium': '', 'brutal': ''}[model_name]\n",
    "            print(\"=\"*80)\n",
    "            print(f\"{icon} {model_name.upper()} ROASTER\")\n",
    "            print(\"=\"*80)\n",
    "            print(critiques[model_name])\n",
    "            print(\"\\n\" * 2)\n",
    "    else:\n",
    "        icon = {'gentle': '', 'medium': '', 'brutal': ''}[ROASTER_MODEL]\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{icon} {ROASTER_MODEL.upper()} ROASTER CRITIQUE\")\n",
    "        print(\"=\"*80)\n",
    "        print(critiques[ROASTER_MODEL])\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\" Critique(s) generated!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating critique(s)...\n",
      "\n",
      "Generating gentle critique...\n",
      "Generating medium critique...\n",
      "Generating brutal critique...\n",
      "================================================================================\n",
      " BRUTAL ROASTER CRITIQUE\n",
      "================================================================================\n",
      "Alright Rafael, buckle up buttercup, because your CV is about to get roasted hotter than a Wiener Schnitzel in July.\n",
      "\n",
      "**OPENING ROAST:**\n",
      "\n",
      "\"Rafael Kacsics, Beruferfahrung\"... Sounds like someone's trying too hard to be German. Did you just Google Translate \"Work Experience\" and slap it on there? I mean, commitment to the theme is admirable, but let's be real, this is less \"efficient German engineering\" and more \"poorly translated instruction manual from IKEA.\"\n",
      "\n",
      "**CAREER OBJECTIVE AUTOPSY:**\n",
      "\n",
      "Oh wait, you *don't* have a career objective. Bold move, Cotton. Let me guess, your life plan involves drifting aimlessly through entry-level positions until you accidentally stumble into a gold mine? Well, good luck with that, buddy. You're gonna need it.\n",
      "\n",
      "**SKILLS COMEDY:**\n",
      "\n",
      "\"WaWI - Systeme.\" Is that some rare disease you caught on your travels? And \"Organisation\"... Congratulations, you can alphabetize your spice rack. How about we list skills that are actually impressive, instead of things you should already know how to do, you doofus.\n",
      "\n",
      "**EXPERIENCE CHECK:**\n",
      "\n",
      "Junior Trader at GVS Austria? Sounds impressive... until you realize it only lasted 6 months. What happened, Rafael? Did you accidentally sell all their gold to a Nigerian prince? Then you were a waiter at Ignaz & Rosalia... What's next, are you gonna tell me you juggled flaming torches in a circus? Pick a career path, any career path. This CV looks like a drunken game of career Twister.\n",
      "\n",
      "**FATAL FLAWS:**\n",
      "\n",
      "*   **The Formatting:** Why is your contact info scattered like confetti after a particularly messy parade?\n",
      "*   **The \"Interessen\":** \"Kraft- & Fitnesstraining\" and \"Design & Mode\"? Are you trying to be the most basic man alive? At least admit you spend your weekends binging Netflix and eating instant noodles.\n",
      "*   **Two Bachelor Degrees (Almost):** You started TWO bachelor degrees and didn't finish either?! What exactly are you planning on doing with your life?\n",
      "\n",
      "**MIC DROP:**\n",
      "\n",
      "Rafael, this CV reads like a cry for help disguised as a resume. You've got potential, but right now, you're like a fine wine... that's still fermenting in the basement and hasn't quite figured out what it wants to be. Get your act together, pick a direction, and for the love of Mozart, hire a professional to rewrite this thing before it gives someone a stroke. Good luck, you magnificent disaster. You're gonna need it.\n",
      "\n",
      "\n",
      "\n",
      " Critique(s) generated!\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation Metrics \n",
    "\n",
    "**Precision, Recall, and F1 Scores:**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T23:25:40.581601Z",
     "start_time": "2025-11-23T23:25:40.563811Z"
    }
   },
   "source": [
    "if cv_text and 'critiques' in locals():\n",
    "    print(\" CALCULATING METRICS...\\n\")\n",
    "    \n",
    "    # Evaluate all models\n",
    "    df_results = processor.evaluate_all_models(cv_text, critiques)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUATION RESULTS - ALL MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n Performance Metrics:\\n\")\n",
    "    print(df_results[['model', 'precision', 'recall', 'f1_score', 'coverage_rate']].to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_idx = df_results['f1_score'].idxmax()\n",
    "    best_model = df_results.loc[best_idx, 'model']\n",
    "    best_f1 = df_results.loc[best_idx, 'f1_score']\n",
    "    \n",
    "    print(f\"\\n Best Model: {best_model.upper()} (F1: {best_f1:.2%})\")\n",
    "    \n",
    "    # Detailed analysis for selected or best model\n",
    "    if ROASTER_MODEL == 'all':\n",
    "        analysis_model = best_model\n",
    "    else:\n",
    "        analysis_model = ROASTER_MODEL\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DETAILED ANALYSIS - {analysis_model.upper()} MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    detection = processor.calculate_issue_detection_metrics(cv_text, critiques[analysis_model])\n",
    "    coverage = processor.calculate_section_coverage(cv_text, critiques[analysis_model])\n",
    "    \n",
    "    print(f\"\\n Issue Detection Quality:\")\n",
    "    print(f\"   Precision:  {detection['precision']:.2%}  (How accurate are the critique's claims?)\")\n",
    "    print(f\"   Recall:     {detection['recall']:.2%}  (How many real issues were caught?)\")\n",
    "    print(f\"   F1 Score:   {detection['f1_score']:.2%}  (Overall balance)\")\n",
    "    \n",
    "    print(f\"\\n Confusion Matrix:\")\n",
    "    print(f\"   True Positives:  {detection['true_positives']}   (Correctly identified issues)\")\n",
    "    print(f\"   False Positives: {detection['false_positives']}    (False alarms)\")\n",
    "    print(f\"   False Negatives: {detection['false_negatives']}   (Missed issues)\")\n",
    "    \n",
    "    print(f\"\\n Issue Breakdown:\")\n",
    "    print(f\"   Actual CV Issues (ground truth):  {detection['ground_truth_issues']}\")\n",
    "    print(f\"   Issues mentioned in critique:     {detection['detected_issues']}\")\n",
    "    \n",
    "    if detection['missed_issues']:\n",
    "        print(f\"     Issues MISSED by critique:     {detection['missed_issues']}\")\n",
    "    \n",
    "    if detection['extra_mentions']:\n",
    "        print(f\"   ℹ  Extra issues mentioned:         {detection['extra_mentions']}\")\n",
    "    \n",
    "    print(f\"\\n Section Coverage:\")\n",
    "    print(f\"   Coverage Rate: {coverage['coverage_rate']:.2%}\")\n",
    "    print(f\"   Sections Addressed: {coverage['sections_addressed_in_critique']}/{coverage['total_sections_in_cv']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"\\n INTERPRETATION:\")\n",
    "    if detection['f1_score'] >= 0.7:\n",
    "        print(\"    GOOD: This critique has high quality (F1 ≥ 0.7)\")\n",
    "    elif detection['f1_score'] >= 0.5:\n",
    "        print(\"     ACCEPTABLE: Decent quality but could be better (F1 = 0.5-0.7)\")\n",
    "    else:\n",
    "        print(\"    POOR: Low quality critique (F1 < 0.5)\")\n",
    "    \n",
    "    if detection['precision'] < 0.6:\n",
    "        print(\"     Low precision: Critique may be making up issues\")\n",
    "    if detection['recall'] < 0.6:\n",
    "        print(\"     Low recall: Critique is missing important issues\")\n",
    "    if coverage['coverage_rate'] >= 0.8:\n",
    "        print(\"    Excellent coverage: Most CV sections were reviewed\")\n",
    "    \n",
    "else:\n",
    "    print(\" Run Step 5 first to generate critiques\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CALCULATING METRICS...\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - ALL MODELS\n",
      "================================================================================\n",
      "\n",
      " Performance Metrics:\n",
      "\n",
      " model  precision  recall  f1_score  coverage_rate\n",
      "gentle   0.400000     1.0  0.571429              0\n",
      "medium   0.333333     1.0  0.500000              0\n",
      "brutal   1.000000     0.5  0.666667              0\n",
      "\n",
      " Best Model: BRUTAL (F1: 66.67%)\n",
      "\n",
      "================================================================================\n",
      "DETAILED ANALYSIS - BRUTAL MODEL\n",
      "================================================================================\n",
      "\n",
      " Issue Detection Quality:\n",
      "   Precision:  100.00%  (How accurate are the critique's claims?)\n",
      "   Recall:     50.00%  (How many real issues were caught?)\n",
      "   F1 Score:   66.67%  (Overall balance)\n",
      "\n",
      " Confusion Matrix:\n",
      "   True Positives:  1   (Correctly identified issues)\n",
      "   False Positives: 0    (False alarms)\n",
      "   False Negatives: 1   (Missed issues)\n",
      "\n",
      " Issue Breakdown:\n",
      "   Actual CV Issues (ground truth):  ['no_metrics', 'formatting']\n",
      "   Issues mentioned in critique:     ['formatting']\n",
      "     Issues MISSED by critique:     ['no_metrics']\n",
      "\n",
      " Section Coverage:\n",
      "   Coverage Rate: 0.00%\n",
      "   Sections Addressed: 0/0\n",
      "\n",
      "================================================================================\n",
      "\n",
      " INTERPRETATION:\n",
      "     ACCEPTABLE: Decent quality but could be better (F1 = 0.5-0.7)\n",
      "     Low recall: Critique is missing important issues\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Quick Summary\n",
    "\n",
    "### How to Use This Notebook:\n",
    "\n",
    "1. **Setup** (Step 1-2): Import libraries and add API key\n",
    "2. **Input CV** (Step 3):\n",
    "   - **Option A**: Put PDF in `uploaded_cvs/` folder, enter filename\n",
    "   - **Option B**: Load existing CV from dataset\n",
    "3. **Choose Model** (Step 4): Set `ROASTER_MODEL` variable\n",
    "4. **Generate** (Step 5): Get your roast!\n",
    "5. **Metrics** (Step 6): See precision, recall, F1 scores\n",
    "\n",
    "\n",
    "### Understanding Metrics:\n",
    "\n",
    "| Metric | What it means | Good Score |\n",
    "|--------|---------------|------------|\n",
    "| **Precision** | % of mentioned issues that are real | > 0.7 |\n",
    "| **Recall** | % of real issues that were caught | > 0.7 |\n",
    "| **F1 Score** | Overall quality (balance of both) | > 0.7 |\n",
    "| **Coverage** | % of CV sections reviewed | > 0.8 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
