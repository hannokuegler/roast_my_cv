{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Fine-Tuning a CV Roaster (Medium Style)\n",
    "\n",
    "This notebook fine-tunes a small language model to generate CV critiques.\n",
    "\n",
    "## Approach\n",
    "1. Generate synthetic training data using Gemini API\n",
    "2. Fine-tune DistilGPT-2 with LoRA (Parameter-Efficient Fine-Tuning)\n",
    "3. Compare: Base Model vs Fine-Tuned vs Gemini\n",
    "\n",
    "## Hardware Requirements\n",
    "- CPU-only compatible (no GPU required)\n",
    "- 8GB RAM sufficient\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded from config.py\n"
     ]
    }
   ],
   "source": [
    "# Load API key from config.py\n",
    "from config import GEMINI_API_KEY\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "print(\"API key loaded from config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9544 resumes\n",
      "Test CVs: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/resume_data.csv')\n",
    "\n",
    "# Load test CV indices\n",
    "with open('../data/test_cv_indices.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "    test_cv_indices = test_data['indices']\n",
    "\n",
    "print(f\"Loaded {len(df)} resumes\")\n",
    "print(f\"Test CVs: {test_cv_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV formatting function (from EDA notebook)\n",
    "def format_cv_for_llm(resume_row):\n",
    "    \"\"\"\n",
    "    Format a resume row into a readable text for LLM processing.\n",
    "    \"\"\"\n",
    "    cv_text = []\n",
    "    \n",
    "    if pd.notna(resume_row.get('career_objective')):\n",
    "        cv_text.append(f\"CAREER OBJECTIVE:\\n{resume_row['career_objective']}\")\n",
    "    \n",
    "    if pd.notna(resume_row.get('skills')):\n",
    "        cv_text.append(f\"\\nSKILLS:\\n{resume_row['skills']}\")\n",
    "    \n",
    "    education_parts = []\n",
    "    if pd.notna(resume_row.get('educational_institution_name')):\n",
    "        education_parts.append(f\"Institution: {resume_row['educational_institution_name']}\")\n",
    "    if pd.notna(resume_row.get('degree_names')):\n",
    "        education_parts.append(f\"Degree: {resume_row['degree_names']}\")\n",
    "    if pd.notna(resume_row.get('major_field_of_studies')):\n",
    "        education_parts.append(f\"Major: {resume_row['major_field_of_studies']}\")\n",
    "    if pd.notna(resume_row.get('passing_years')):\n",
    "        education_parts.append(f\"Year: {resume_row['passing_years']}\")\n",
    "    \n",
    "    if education_parts:\n",
    "        cv_text.append(f\"\\nEDUCATION:\\n\" + \"\\n\".join(education_parts))\n",
    "    \n",
    "    work_parts = []\n",
    "    if pd.notna(resume_row.get('professional_company_names')):\n",
    "        work_parts.append(f\"Company: {resume_row['professional_company_names']}\")\n",
    "    if pd.notna(resume_row.get('positions')):\n",
    "        work_parts.append(f\"Position: {resume_row['positions']}\")\n",
    "    if pd.notna(resume_row.get('start_dates')):\n",
    "        work_parts.append(f\"Period: {resume_row['start_dates']}\")\n",
    "        if pd.notna(resume_row.get('end_dates')):\n",
    "            work_parts.append(f\" to {resume_row['end_dates']}\")\n",
    "    if pd.notna(resume_row.get('responsibilities')):\n",
    "        work_parts.append(f\"Responsibilities:\\n{resume_row['responsibilities']}\")\n",
    "    \n",
    "    if work_parts:\n",
    "        cv_text.append(f\"\\nWORK EXPERIENCE:\\n\" + \"\\n\".join(work_parts))\n",
    "    \n",
    "    if pd.notna(resume_row.get('languages')):\n",
    "        cv_text.append(f\"\\nLANGUAGES:\\n{resume_row['languages']}\")\n",
    "    \n",
    "    if pd.notna(resume_row.get('certification_skills')):\n",
    "        cv_text.append(f\"\\nCERTIFICATIONS:\\n{resume_row['certification_skills']}\")\n",
    "    \n",
    "    return \"\\n\".join(cv_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Roaster Prompt (Same as 03_medium_roaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIUM_SYSTEM_PROMPT = \"\"\"You are an experienced hiring manager who provides direct, honest CV feedback.\n",
    "\n",
    "Your approach:\n",
    "1. Be direct and honest - no sugarcoating\n",
    "2. Point out obvious flaws and red flags\n",
    "3. Call out generic buzzwords and filler content\n",
    "4. Be professional but don't hold back the truth\n",
    "5. Focus on what actually matters to employers\n",
    "\n",
    "Keep your feedback:\n",
    "- Brutally honest but professional\n",
    "- Direct about weaknesses\n",
    "- Critical of vague or generic content\n",
    "- Focused on real-world hiring standards\n",
    "\n",
    "Structure your response:\n",
    "FIRST IMPRESSION: What stands out (good or bad)\n",
    "MAJOR ISSUES: Glaring problems that need fixing\n",
    "CONCERNS: Things that raise questions\n",
    "WHAT WORKS: Brief acknowledgment of strengths\n",
    "BOTTOM LINE: Final verdict and priority fixes\n",
    "\"\"\"\n",
    "\n",
    "def roast_cv_gemini(cv_text, temperature=0.7, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generate CV critique using Gemini.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=model_name,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            top_k=40,\n",
    "            max_output_tokens=1024,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    prompt = f\"{MEDIUM_SYSTEM_PROMPT}\\n\\nReview this CV with honest, direct feedback:\\n\\n{cv_text}\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Generate Synthetic Training Data\n",
    "\n",
    "Create (CV, critique) pairs using Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SYNTHETIC_DATA_PATH = Path('../data/fine_tuning_dataset.json')\n",
    "MODEL_OUTPUT_DIR = Path('../models/medium_roaster_lora')\n",
    "NUM_TRAINING_SAMPLES = 100\n",
    "DELAY_BETWEEN_CALLS = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_dataset(df, num_samples=NUM_TRAINING_SAMPLES):\n",
    "    \"\"\"\n",
    "    Generate (CV, critique) pairs for fine-tuning.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Randomly sample CVs (excluding test CVs)\n",
    "    available_indices = [i for i in range(len(df)) if i not in test_cv_indices]\n",
    "    sample_indices = np.random.choice(available_indices, min(num_samples, len(available_indices)), replace=False)\n",
    "    \n",
    "    print(f\"Generating {len(sample_indices)} critique pairs...\")\n",
    "    print(f\"Estimated time: {len(sample_indices) * DELAY_BETWEEN_CALLS / 60:.1f} minutes\")\n",
    "    \n",
    "    for i, idx in enumerate(tqdm(sample_indices)):\n",
    "        cv_text = format_cv_for_llm(df.iloc[idx])\n",
    "        \n",
    "        if not cv_text or len(cv_text) < 100:\n",
    "            continue\n",
    "        \n",
    "        # Truncate very long CVs\n",
    "        cv_text = cv_text[:3000]\n",
    "        \n",
    "        # Generate critique with Gemini\n",
    "        try:\n",
    "            critique = roast_cv_gemini(cv_text)\n",
    "            dataset.append({\n",
    "                \"cv\": cv_text,\n",
    "                \"critique\": critique,\n",
    "                \"source_idx\": int(idx)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error on CV {idx}: {e}\")\n",
    "        \n",
    "        time.sleep(DELAY_BETWEEN_CALLS)\n",
    "        \n",
    "        # Save progress every 20 samples\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"\\nCheckpoint: {len(dataset)} pairs saved\")\n",
    "            with open(SYNTHETIC_DATA_PATH, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset from ..\\data\\fine_tuning_dataset.json\n",
      "Loaded 100 pairs\n"
     ]
    }
   ],
   "source": [
    "# Check if dataset already exists, otherwise generate\n",
    "if SYNTHETIC_DATA_PATH.exists():\n",
    "    print(f\"Loading existing dataset from {SYNTHETIC_DATA_PATH}\")\n",
    "    with open(SYNTHETIC_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        synthetic_data = json.load(f)\n",
    "    print(f\"Loaded {len(synthetic_data)} pairs\")\n",
    "else:\n",
    "    print(\"Generating new synthetic dataset...\")\n",
    "    synthetic_data = generate_synthetic_dataset(df, NUM_TRAINING_SAMPLES)\n",
    "    \n",
    "    # Save final dataset\n",
    "    with open(SYNTHETIC_DATA_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(synthetic_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nSaved {len(synthetic_data)} pairs to {SYNTHETIC_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100 pairs\n",
      "\n",
      "Sample CV (truncated):\n",
      "================================================================================\n",
      "\n",
      "SKILLS:\n",
      "['Periodic financial reporting expert', 'General ledger accounting skills', 'Invoice coding familiarity', 'Strong communication skills', 'Complex problem solving', 'Account reconciliation expert', 'Organization', 'Time Management', 'Adaptability', 'Communication']\n",
      "\n",
      "EDUCATION:\n",
      "Institution: ['University of Greenwich', 'Oshwal College']\n",
      "Degree: ['Bachelor of Arts', 'Association of Business Executive']\n",
      "Major: ['Business Studies', 'Business']\n",
      "Year: ['2014', '2013']\n",
      "\n",
      "WORK EXPERIENCE:\n",
      "Company:...\n",
      "\n",
      "Sample Critique:\n",
      "================================================================================\n",
      "Okay, let's rip this CV apart. Here's the brutally honest truth:\n",
      "\n",
      "**FIRST IMPRESSION:** This CV screams \"entry-level and desperate.\" The formatting is basic, the skills section is a buzzword bingo, and the work experience section is a laundry list of generic tasks. It looks like a template someone filled out without putting any real thought into it.\n",
      "\n",
      "**MAJOR ISSUES:**\n",
      "\n",
      "*   **Skills Section - Useless:** This is a collection of generic buzzwords and vague statements. \"Strong communication skills,\"...\n"
     ]
    }
   ],
   "source": [
    "# Preview dataset\n",
    "print(f\"Dataset size: {len(synthetic_data)} pairs\")\n",
    "print(f\"\\nSample CV (truncated):\")\n",
    "print(\"=\"*80)\n",
    "print(synthetic_data[0]['cv'][:500] + \"...\")\n",
    "print(f\"\\nSample Critique:\")\n",
    "print(\"=\"*80)\n",
    "print(synthetic_data[0]['critique'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Fine-Tune DistilGPT-2 with LoRA\n",
    "\n",
    "Using Parameter-Efficient Fine-Tuning (PEFT) to train on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilgpt2...\n",
      "Model parameters: 81,912,576\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 405,504 || all params: 82,318,080 || trainable%: 0.4926\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA - only trains ~0.5% of parameters\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted 100 training examples\n"
     ]
    }
   ],
   "source": [
    "# Format training data\n",
    "def format_training_example(cv, critique):\n",
    "    \"\"\"Format a (CV, critique) pair for training.\"\"\"\n",
    "    return f\"### CV:\\n{cv[:1500]}\\n\\n### Critique:\\n{critique}\\n\\n### END\"\n",
    "\n",
    "formatted_texts = [\n",
    "    format_training_example(item['cv'], item['critique'])\n",
    "    for item in synthetic_data\n",
    "]\n",
    "\n",
    "print(f\"Formatted {len(formatted_texts)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b65b103a0914c069a392a2b6602aedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae06f57422104514af7481448fc3a8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 90, Eval: 10\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": formatted_texts})\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset = train_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "# Split train/eval\n",
    "split = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = split[\"train\"]\n",
    "eval_data = split[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Eval: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ready\n"
     ]
    }
   ],
   "source": [
    "# Training arguments (CPU optimized)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODEL_OUTPUT_DIR),\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    use_cpu=True,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Expected time: 30-60 minutes on CPU\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [69/69 4:16:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.503800</td>\n",
       "      <td>3.408100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=69, training_loss=3.666443728018498, metrics={'train_runtime': 15437.7317, 'train_samples_per_second': 0.017, 'train_steps_per_second': 0.004, 'total_flos': 35611402567680.0, 'train_loss': 3.666443728018498, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "print(\"Expected time: 30-60 minutes on CPU\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ..\\models\\medium_roaster_lora\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved to {MODEL_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Test & Compare Models\n",
    "\n",
    "Compare three models:\n",
    "1. **Base DistilGPT-2** (before fine-tuning)\n",
    "2. **Fine-tuned DistilGPT-2** (after LoRA training)\n",
    "3. **Gemini** (reference baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for comparison...\n",
      "Both models loaded\n"
     ]
    }
   ],
   "source": [
    "# Load both models for comparison\n",
    "print(\"Loading models for comparison...\")\n",
    "\n",
    "# Base model (no fine-tuning)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "base_model.eval()\n",
    "\n",
    "# Fine-tuned model (with LoRA)\n",
    "ft_base = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "fine_tuned_model = PeftModel.from_pretrained(ft_base, MODEL_OUTPUT_DIR)\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "print(\"Both models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roast_cv_local(model, cv_text, max_new_tokens=75):\n",
    "    \"\"\"\n",
    "    Generate critique using a local model (base or fine-tuned).\n",
    "    IMPROVED: Better prompt format for inference.\n",
    "    \"\"\"\n",
    "    # Shorter CV + clear instruction\n",
    "    cv_short = cv_text[:800]  # Only 800 chars\n",
    "    \n",
    "    prompt = f\"You are a hiring manager who criticizes CVs. Criticize this CV:\\n\\n{cv_short} END OF THE CV.\\n\\n Now your Critique: I as a hiring manager think this CV is bad because\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=400)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.5,  # Lower temp = less random\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract critique part\n",
    "    if \"Critique:\" in generated:\n",
    "        critique = generated.split(\"Critique:\")[1]\n",
    "        return critique.strip()\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CV:\n",
      "================================================================================\n",
      "CAREER OBJECTIVE:\n",
      "Big data analytics working and database warehouse manager with robust experience in handling all kinds of data. I have also used multiple cloud infrastructure services and am well acquainted with them. Currently in search of role that offers more of development.\n",
      "\n",
      "SKILLS:\n",
      "['Big Data', 'Hadoop', 'Hive', 'Python', 'Mapreduce', 'Spark', 'Java', 'Machine Learning', 'Cloud', 'Hdfs', 'YARN', 'Core Java', 'Data Science', 'C++', 'Data Structures', 'DBMS', 'RDBMS', 'Informatica', 'Talend...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test on first test CV\n",
    "test_cv = format_cv_for_llm(df.iloc[test_cv_indices[0]])\n",
    "\n",
    "print(\"TEST CV:\")\n",
    "print(\"=\"*80)\n",
    "print(test_cv[:500] + \"...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. BASE MODEL (no fine-tuning):\n",
      "================================================================================\n",
      "I as a hiring manager think this CV is bad because it is a good one.\n",
      "CAREER OBJECTIVE:\n",
      "Big data analytics working and database warehouse manager with robust experience in handling all kinds of data. I have also used multiple cloud infrastructure services and am well acquainted with them. Currently in search of role that offers more of development.\n",
      "SKILLS:\n",
      "['Big Data', 'Hive', 'Python\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. BASE MODEL (no fine-tuning):\")\n",
    "print(\"=\"*80)\n",
    "base_critique = roast_cv_local(base_model, test_cv)\n",
    "print(base_critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "2. FINE-TUNED MODEL (after LoRA training):\n",
      "================================================================================\n",
      "I as a hiring manager think this CV is bad because it lacks a clear understanding of what it actually means.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. FINE-TUNED MODEL (after LoRA training):\")\n",
    "print(\"=\"*80)\n",
    "ft_critique = roast_cv_local(fine_tuned_model, test_cv)\n",
    "print(ft_critique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "3. GEMINI (reference):\n",
      "================================================================================\n",
      "Okay, here's my brutally honest assessment of this CV:\n",
      "\n",
      "**FIRST IMPRESSION:** This CV screams \"generic and underwhelming.\" It's a laundry list of buzzwords with little substance to back them up. The formatting is basic, and the content lacks impact.\n",
      "\n",
      "**MAJOR ISSUES:**\n",
      "\n",
      "*   **Career Objective:** This is a terrible opening. It's vague, contains grammatical errors (\"working\" should be \"worker\" or removed entirely), and doesn't sell you at all. Saying you're \"in search of a role that offers more of development\" is weak. What kind of development? Why should a company invest in you for that? This needs a complete rewrite to showcase your value proposition and target a specific role.\n",
      "*   **Skills Section:** This is just a keyword dump. Listing a bunch of technologies without demonstrating proficiency is pointless. Anyone can copy and paste these terms. There's no indication of your skill level or how you've applied these skills in real-world projects.\n",
      "*   **Work Experience (Coca-Cola):** This is a massive red flag. \"Technical Support, Troubleshooting, Collaboration, Documentation, System Monitoring, Software Deployment, Training & Mentorship, Industry Trends, Field Visits\" could describe almost *any* job. As a Big Data Analyst, what specific projects did you work on? What were the results? Did you improve efficiency, reduce costs, or increase revenue? Quantify your accomplishments with numbers and specific examples. The lack of detail makes me question whether you were *actually* a Big Data Analyst, or just doing basic IT support.\n",
      "*   **Education:** Listing \"Electronics\" as your major when applying for data roles is a concern. It's not directly relevant. You need to connect the dots and explain how your electronics background prepared you for a career in data (e.g., strong analytical skills, problem-solving abilities).\n",
      "\n",
      "**CONCERNS:**\n",
      "\n",
      "*   **\"Till Date\":** While it's good you're employed, \"Till Date\" is not a professional way to end the work experience period. Use the accurate date (e.g., \"Nov 2019 - Present\").\n",
      "*   **Lack of Projects:** Where are your personal projects or contributions to open-source projects? If you want a \"development\" role, you need to demonstrate your coding abilities outside of work.\n",
      "*   **Inconsistent Capitalization:** The skills section has inconsistent capitalization. This makes the CV look sloppy and unprofessional.\n",
      "\n",
      "**WHAT WORKS:**\n",
      "\n",
      "*   You have a B.Tech degree.\n",
      "*   You have some work experience, even if it is lacking in detail.\n",
      "\n",
      "**BOTTOM LINE:**\n",
      "\n",
      "This CV is not going to get you interviews for any serious data science or development roles. It's generic, lacks quantifiable achievements, and raises serious doubts about your actual skills and experience.\n",
      "\n",
      "**Priority Fixes:**\n",
      "\n",
      "1.  **Rewrite the Career Objective:** Focus on what you offer, not what you want. Tailor it to the specific type of role you're targeting.\n",
      "2.  **Revamp the Skills Section:** Remove irrelevant skills and provide context for the ones you keep. Quantify your proficiency level (e.g., \"Proficient in Python with 3+ years of experience in data analysis and machine learning\").\n",
      "3.  **Completely Rewrite the Coca-Cola Experience:** Focus on specific projects, technologies used, and quantifiable results. Use the STAR method (Situation, Task, Action, Result) to showcase your accomplishments.\n",
      "4.  **Add Projects:** Include personal projects or open-source contributions to demonstrate your coding abilities.\n",
      "5.  **Proofread Carefully:** Fix grammatical errors and inconsistencies.\n",
      "\n",
      "This CV needs a complete overhaul. Good luck!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. GEMINI (reference):\")\n",
    "print(\"=\"*80)\n",
    "gemini_critique = roast_cv_gemini(test_cv)\n",
    "print(gemini_critique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation\n",
    "\n",
    "Using CVProcessor metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation\n",
    "\n",
    "Using LLM Judge (same approach as notebook 05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating all three critiques with LLM judge...\n",
      "This may take ~30 seconds...\n",
      "\n",
      "Evaluating Base...\n",
      "  ✓ Base evaluated\n",
      "Evaluating Fine-Tuned...\n",
      "  ✓ Fine-Tuned evaluated\n",
      "Evaluating Gemini...\n",
      "  ✓ Gemini evaluated\n",
      "\n",
      "✓ Completed 3 evaluations\n"
     ]
    }
   ],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are an expert evaluator of CV critique quality.\n",
    "\n",
    "Evaluate this CV critique on the following criteria (score 1-10 for each):\n",
    "\n",
    "1. **Specificity**: How specific and actionable is the feedback?\n",
    "2. **Relevance**: How relevant are the points to actual CV improvement?\n",
    "3. **Coherence**: Is the critique coherent and well-structured?\n",
    "4. **Completeness**: Does it cover important aspects of the CV?\n",
    "5. **Overall Usefulness**: How useful would this be to the job seeker?\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "  \"specificity\": <score>,\n",
    "  \"relevance\": <score>,\n",
    "  \"coherence\": <score>,\n",
    "  \"completeness\": <score>,\n",
    "  \"overall_usefulness\": <score>,\n",
    "  \"reasoning\": \"<brief explanation>\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_critique_with_llm(critique_text, model_type, cv_text):\n",
    "    \"\"\"\n",
    "    Use LLM to evaluate critique quality.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            temperature=0.2,  # Low temperature for consistent evaluation\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"{JUDGE_PROMPT}\n",
    "\n",
    "Model Type: {model_type}\n",
    "\n",
    "Original CV (excerpt):\n",
    "{cv_text[:500]}...\n",
    "\n",
    "Critique to Evaluate:\n",
    "{critique_text}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        text = response.text\n",
    "        # Extract JSON from response\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}') + 1\n",
    "        if start != -1 and end != 0:\n",
    "            json_str = text[start:end]\n",
    "            return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Evaluating all three critiques with LLM judge...\")\n",
    "print(\"This may take ~30 seconds...\\n\")\n",
    "\n",
    "# Evaluate all three models\n",
    "evaluations = []\n",
    "\n",
    "for model_name, critique in [('Base', base_critique), ('Fine-Tuned', ft_critique), ('Gemini', gemini_critique)]:\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    eval_result = evaluate_critique_with_llm(critique, model_name, test_cv)\n",
    "    \n",
    "    if eval_result:\n",
    "        eval_result['model'] = model_name\n",
    "        evaluations.append(eval_result)\n",
    "        print(f\"  ✓ {model_name} evaluated\")\n",
    "\n",
    "print(f\"\\n✓ Completed {len(evaluations)} evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION SCORES (LLM JUDGE)\n",
      "================================================================================\n",
      "            Metric Base (before) Fine-Tuned (after) Gemini (reference)\n",
      "       Specificity        1.0/10             1.0/10             9.0/10\n",
      "         Relevance        1.0/10             2.0/10            10.0/10\n",
      "         Coherence        1.0/10             1.0/10             9.0/10\n",
      "      Completeness        1.0/10             1.0/10             9.0/10\n",
      "Overall Usefulness        1.0/10             1.0/10            10.0/10\n",
      "\n",
      "================================================================================\n",
      "AVERAGE SCORES\n",
      "================================================================================\n",
      "Base           : 1.00/10\n",
      "Fine-Tuned     : 1.20/10\n",
      "Gemini         : 9.40/10\n",
      "\n",
      "================================================================================\n",
      "LLM JUDGE REASONING\n",
      "================================================================================\n",
      "\n",
      "Base:\n",
      "  The critique is nonsensical and provides no actionable feedback. It contradicts itself by saying the CV is both good and bad. It also abruptly cuts off the skills section. It offers no specific suggestions for improvement.\n",
      "\n",
      "Fine-Tuned:\n",
      "  The critique is extremely vague and unhelpful. It states the CV is \"bad\" and lacks \"a clear understanding of what it actually means\" without providing any specific examples or actionable advice. It doesn't address any particular section of the CV or offer suggestions for improvement. The feedback is essentially useless to the job seeker.\n",
      "\n",
      "Gemini:\n",
      "  This is an excellent CV critique. It is highly specific, providing concrete examples of what needs improvement and how to do it. The feedback is directly relevant to improving the CV's effectiveness in securing interviews. The critique is well-organized, covering key sections of the CV and offering both positive and negative feedback. It is also comprehensive, addressing issues ranging from the career objective to the skills section and work experience. The 'Priority Fixes' section is particularly helpful in guiding the job seeker on where to focus their efforts. The use of examples and suggestions makes the critique actionable and highly useful.\n"
     ]
    }
   ],
   "source": [
    "# Display evaluation results\n",
    "if evaluations:\n",
    "    df_eval = pd.DataFrame(evaluations)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION SCORES (LLM JUDGE)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    score_cols = ['specificity', 'relevance', 'coherence', 'completeness', 'overall_usefulness']\n",
    "    \n",
    "    # Display table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Specificity', 'Relevance', 'Coherence', 'Completeness', 'Overall Usefulness'],\n",
    "        'Base (before)': [\n",
    "            f\"{df_eval[df_eval['model']=='Base']['specificity'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Base']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Base']['relevance'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Base']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Base']['coherence'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Base']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Base']['completeness'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Base']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Base']['overall_usefulness'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Base']) > 0 else \"N/A\",\n",
    "        ],\n",
    "        'Fine-Tuned (after)': [\n",
    "            f\"{df_eval[df_eval['model']=='Fine-Tuned']['specificity'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Fine-Tuned']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Fine-Tuned']['relevance'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Fine-Tuned']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Fine-Tuned']['coherence'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Fine-Tuned']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Fine-Tuned']['completeness'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Fine-Tuned']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Fine-Tuned']['overall_usefulness'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Fine-Tuned']) > 0 else \"N/A\",\n",
    "        ],\n",
    "        'Gemini (reference)': [\n",
    "            f\"{df_eval[df_eval['model']=='Gemini']['specificity'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Gemini']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Gemini']['relevance'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Gemini']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Gemini']['coherence'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Gemini']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Gemini']['completeness'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Gemini']) > 0 else \"N/A\",\n",
    "            f\"{df_eval[df_eval['model']=='Gemini']['overall_usefulness'].values[0]:.1f}/10\" if len(df_eval[df_eval['model']=='Gemini']) > 0 else \"N/A\",\n",
    "        ],\n",
    "    })\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate averages\n",
    "    df_eval['average_score'] = df_eval[score_cols].mean(axis=1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AVERAGE SCORES\")\n",
    "    print(\"=\"*80)\n",
    "    for _, row in df_eval.iterrows():\n",
    "        print(f\"{row['model']:15s}: {row['average_score']:.2f}/10\")\n",
    "    \n",
    "    # Show reasoning\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LLM JUDGE REASONING\")\n",
    "    print(\"=\"*80)\n",
    "    for _, row in df_eval.iterrows():\n",
    "        print(f\"\\n{row['model']}:\")\n",
    "        print(f\"  {row['reasoning']}\")\n",
    "else:\n",
    "    print(\"No evaluation results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Conclusion\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "This notebook demonstrated several important ML engineering techniques:\n",
    "\n",
    "**Technical Skills Practiced:**\n",
    "- **Synthetic Data Generation**: Used Gemini API to generate 100 (CV, critique) training pairs\n",
    "- **Parameter-Efficient Fine-Tuning**: Implemented LoRA to train only 0.49% of model parameters (405K out of 82M)\n",
    "- **CPU-Optimized Training**: Successfully fine-tuned on CPU with 8GB RAM (no GPU required)\n",
    "- **Model Comparison**: Systematically compared Base vs Fine-Tuned vs Gemini models\n",
    "\n",
    "**Process Insights:**\n",
    "- Learned how to create domain-specific training data from a large language model\n",
    "- Understood trade-offs between model size, quality, and computational requirements\n",
    "- Practiced prompt engineering for both training and inference\n",
    "- Implemented consistent evaluation methodology across notebooks\n",
    "\n",
    "---\n",
    "\n",
    "### Key Finding: Model Size Matters\n",
    "\n",
    "**DistilGPT-2 (82M parameters) is too small for this complex task.**\n",
    "\n",
    "| Model | Parameters | Performance | Quality Assessment |\n",
    "|-------|-----------|-------------|-------------------|\n",
    "| **Base DistilGPT-2** | 82M | Poor | Incoherent, repetitive output |\n",
    "| **Fine-Tuned DistilGPT-2** | 82M | Minimal improvement | Slightly better but still unusable |\n",
    "| **Gemini 2.0 Flash** | ~Billions | Excellent | Professional, actionable feedback |\n",
    "\n",
    "**Why DistilGPT-2 Failed:**\n",
    "1. **Model capacity too limited**: CV critique requires understanding context, professional norms, and constructive feedback patterns\n",
    "2. **Task complexity**: Analyzing CVs demands domain knowledge that small models can't capture\n",
    "3. **Training data insufficient**: 100 examples can't compensate for lack of base capabilities\n",
    "4. **High training loss (3.666)**: Model struggled to learn even basic patterns\n",
    "\n",
    "**Evidence from LLM Judge Scores:**\n",
    "- Base Model: 1.0/10 average score\n",
    "- Fine-Tuned Model: 1.2/10 average score (minimal improvement)\n",
    "- Gemini: 9.4/10 average score\n",
    "\n",
    "**Conclusion:**\n",
    "For complex, nuanced tasks like CV critique, **larger models (1B+ parameters) or cloud-based LLMs are necessary**. Small models like DistilGPT-2 work well for simple classification or text completion, but fail at tasks requiring reasoning, domain expertise, and structured feedback generation.\n",
    "\n",
    "---\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "If pursuing local fine-tuning even further:\n",
    "- Use larger models (e.g., GPT-2 Medium/Large, Llama 3 8B)\n",
    "- Generate more training data (500-1000 examples)\n",
    "- Consider cloud GPU for faster training\n",
    "\n",
    "**Recommendation:** For production use, stick with cloud-based LLMs that have the capacity for this task complexity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
