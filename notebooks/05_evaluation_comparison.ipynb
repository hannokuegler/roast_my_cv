{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Evaluation & Comparison \n",
    "\n",
    "This notebook compares all three CV roasting models and evaluates their effectiveness.\n",
    "\n",
    "## Objectives\n",
    "- Load results from all three models\n",
    "- Compare roastings side-by-side\n",
    "- Analyze characteristics such as length, tone and specificity\n",
    "- Evaluate effectiveness (using another LLM)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import google.generativeai as genai\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from config.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import GEMINI_API_KEY\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "print(\"API key loaded from config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(model_name):\n",
    "    \"\"\"\n",
    "    Load all results for a specific model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model (gentle_roaster, medium_roaster, brutal_roaster)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of result dictionaries\n",
    "    \"\"\"\n",
    "    results_dir = Path(f'../results/{model_name}')\n",
    "    results = []\n",
    "    \n",
    "    for file_path in sorted(results_dir.glob('*.json')):\n",
    "        with open(file_path, 'r') as f:\n",
    "            results.append(json.load(f))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all results\n",
    "gentle_results = load_results('gentle_roaster')\n",
    "medium_results = load_results('medium_roaster')\n",
    "brutal_results = load_results('brutal_roaster')\n",
    "\n",
    "print(f\"Loaded results:\")\n",
    "print(f\"  Gentle: {len(gentle_results)} critiques\")\n",
    "print(f\"  Medium: {len(medium_results)} critiques\")\n",
    "print(f\"  Brutal: {len(brutal_results)} critiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(cv_index):\n",
    "    \"\"\"\n",
    "    Display all three roasts for a given CV side-by-side.\n",
    "    \"\"\"\n",
    "    print(\"=\"*120)\n",
    "    print(f\"CV #{cv_index} - THREE ROASTING STYLES COMPARISON\")\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    # Find results for this CV\n",
    "    gentle = next((r for r in gentle_results if r['cv_index'] == cv_index), None)\n",
    "    medium = next((r for r in medium_results if r['cv_index'] == cv_index), None)\n",
    "    brutal = next((r for r in brutal_results if r['cv_index'] == cv_index), None)\n",
    "    \n",
    "    if gentle:\n",
    "        print(\"\\n ORIGINAL CV:\")\n",
    "        print(\"-\"*120)\n",
    "        print(gentle['cv_text'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\" GENTLE ROASTER (Temperature: 0.4)\")\n",
    "    print(\"=\"*120)\n",
    "    if gentle:\n",
    "        print(gentle['critique'])\n",
    "    else:\n",
    "        print(\"No results found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\" MEDIUM ROASTER (Temperature: 0.7)\")\n",
    "    print(\"=\"*120)\n",
    "    if medium:\n",
    "        print(medium['critique'])\n",
    "    else:\n",
    "        print(\"No results found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\" BRUTAL ROASTER (Temperature: 0.9)\")\n",
    "    print(\"=\"*120)\n",
    "    if brutal:\n",
    "        print(brutal['critique'])\n",
    "    else:\n",
    "        print(\"No results found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120 + \"\\n\")\n",
    "\n",
    "# Display comparisons for all test CVs\n",
    "if gentle_results:\n",
    "    for result in gentle_results:\n",
    "        display_comparison(result['cv_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_critique(critique_text):\n",
    "    \"\"\"\n",
    "    Analyze characteristics of a critique.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'char_count': len(critique_text),\n",
    "        'word_count': len(critique_text.split()),\n",
    "        'line_count': len(critique_text.split('\\n')),\n",
    "        'avg_word_length': sum(len(word) for word in critique_text.split()) / max(len(critique_text.split()), 1),\n",
    "        'emoji_count': sum(1 for char in critique_text if ord(char) > 0x1F300),\n",
    "    }\n",
    "\n",
    "# Analyze all critiques\n",
    "analysis_data = []\n",
    "\n",
    "for model_name, results in [('Gentle', gentle_results), ('Medium', medium_results), ('Brutal', brutal_results)]:\n",
    "    for result in results:\n",
    "        metrics = analyze_critique(result['critique'])\n",
    "        metrics['model'] = model_name\n",
    "        metrics['cv_index'] = result['cv_index']\n",
    "        metrics['temperature'] = result['temperature']\n",
    "        analysis_data.append(metrics)\n",
    "\n",
    "df_analysis = pd.DataFrame(analysis_data)\n",
    "\n",
    "print(\" CRITIQUE STATISTICS BY MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(df_analysis.groupby('model')[['char_count', 'word_count', 'line_count', 'emoji_count']].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Word count\n",
    "df_analysis.groupby('model')['word_count'].mean().plot(kind='bar', ax=axes[0], color=['#90EE90', '#FFD700', '#FF6347'])\n",
    "axes[0].set_title('Average Word Count by Model', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Words')\n",
    "axes[0].set_xlabel('')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Character count\n",
    "df_analysis.groupby('model')['char_count'].mean().plot(kind='bar', ax=axes[1], color=['#90EE90', '#FFD700', '#FF6347'])\n",
    "axes[1].set_title('Average Character Count by Model', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Characters')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Emoji usage\n",
    "df_analysis.groupby('model')['emoji_count'].mean().plot(kind='bar', ax=axes[2], color=['#90EE90', '#FFD700', '#FF6347'])\n",
    "axes[2].set_title('Average Emoji Usage by Model', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Emojis')\n",
    "axes[2].set_xlabel('')\n",
    "axes[2].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Saved visualization to results/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature vs Output Length\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model in ['Gentle', 'Medium', 'Brutal']:\n",
    "    model_data = df_analysis[df_analysis['model'] == model]\n",
    "    plt.scatter(model_data['temperature'], model_data['word_count'], \n",
    "               label=model, s=100, alpha=0.6)\n",
    "\n",
    "plt.xlabel('Temperature', fontsize=12)\n",
    "plt.ylabel('Word Count', fontsize=12)\n",
    "plt.title('Temperature vs Output Length', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../results/temperature_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Saved visualization to results/temperature_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Evaluation with LLM Judge\n",
    "\n",
    "Use Gemini to evaluate the quality of each roast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are an expert evaluator of CV critique quality.\n",
    "\n",
    "Evaluate this CV critique on the following criteria (score 1-10 for each):\n",
    "\n",
    "1. **Specificity**: How specific and actionable is the feedback?\n",
    "2. **Relevance**: How relevant are the points to actual CV improvement?\n",
    "3. **Tone Appropriateness**: How well does the tone match the intended style?\n",
    "4. **Completeness**: Does it cover all major aspects of the CV?\n",
    "5. **Overall Usefulness**: How useful would this be to the job seeker?\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "  \"specificity\": <score>,\n",
    "  \"relevance\": <score>,\n",
    "  \"tone_appropriateness\": <score>,\n",
    "  \"completeness\": <score>,\n",
    "  \"overall_usefulness\": <score>,\n",
    "  \"reasoning\": \"<brief explanation>\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_critique(critique_text, model_type, cv_text):\n",
    "    \"\"\"\n",
    "    Use LLM to evaluate critique quality.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            temperature=0.2,  # Low temperature for consistent evaluation\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"{JUDGE_PROMPT}\n",
    "\n",
    "Model Type: {model_type}\n",
    "\n",
    "Original CV:\n",
    "{cv_text[:500]}...\n",
    "\n",
    "Critique to Evaluate:\n",
    "{critique_text}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        # Extract JSON from response\n",
    "        text = response.text\n",
    "        # Try to find JSON in the response\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}') + 1\n",
    "        if start != -1 and end != 0:\n",
    "            json_str = text[start:end]\n",
    "            return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Evaluating critiques with LLM judge...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for model_name, results in [('Gentle', gentle_results), ('Medium', medium_results), ('Brutal', brutal_results)]:\n",
    "    for result in results:\n",
    "        print(f\"Evaluating {model_name} model for CV #{result['cv_index']}...\")\n",
    "        eval_result = evaluate_critique(\n",
    "            result['critique'], \n",
    "            model_name,\n",
    "            result['cv_text']\n",
    "        )\n",
    "        \n",
    "        if eval_result:\n",
    "            eval_result['model'] = model_name\n",
    "            eval_result['cv_index'] = result['cv_index']\n",
    "            evaluations.append(eval_result)\n",
    "\n",
    "print(f\"\\n Completed {len(evaluations)} evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "if evaluations:\n",
    "    df_eval = pd.DataFrame(evaluations)\n",
    "    \n",
    "    print(\"\\n EVALUATION SCORES BY MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    score_cols = ['specificity', 'relevance', 'tone_appropriateness', 'completeness', 'overall_usefulness']\n",
    "    summary = df_eval.groupby('model')[score_cols].mean().round(2)\n",
    "    print(summary)\n",
    "    \n",
    "    # Calculate overall average\n",
    "    df_eval['average_score'] = df_eval[score_cols].mean(axis=1)\n",
    "    \n",
    "    print(\"\\n OVERALL AVERAGE SCORES\")\n",
    "    print(df_eval.groupby('model')['average_score'].mean().round(2).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of evaluation scores\n",
    "if evaluations:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    summary.T.plot(kind='bar', ax=ax, color=['#90EE90', '#FFD700', '#FF6347'])\n",
    "    ax.set_title('Evaluation Scores by Model', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Score (1-10)', fontsize=12)\n",
    "    ax.set_xlabel('Evaluation Criteria', fontsize=12)\n",
    "    ax.legend(title='Model', loc='upper right')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, 10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/evaluation_scores.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\" Saved visualization to results/evaluation_scores.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. TEMPERATURE EFFECTS:\")\n",
    "print(f\"   • Gentle (T=0.4): Most consistent, professional feedback\")\n",
    "print(f\"   • Medium (T=0.7): Good balance of directness and variety\")\n",
    "print(f\"   • Brutal (T=0.9): Maximum creativity and humor\")\n",
    "\n",
    "if evaluations:\n",
    "    print(\"\\n2. QUALITY ASSESSMENT:\")\n",
    "    best_model = df_eval.groupby('model')['average_score'].mean().idxmax()\n",
    "    best_score = df_eval.groupby('model')['average_score'].mean().max()\n",
    "    print(f\"   • Best Overall Model: {best_model} (avg score: {best_score:.2f}/10)\")\n",
    "    \n",
    "    for model in ['Gentle', 'Medium', 'Brutal']:\n",
    "        if model in df_eval['model'].values:\n",
    "            model_data = df_eval[df_eval['model'] == model]\n",
    "            print(f\"   • {model}: {model_data['average_score'].mean():.2f}/10\")\n",
    "\n",
    "print(\"\\n3. CHARACTERISTICS:\")\n",
    "print(f\"   • Gentle: {df_analysis[df_analysis['model']=='Gentle']['word_count'].mean():.0f} avg words\")\n",
    "print(f\"   • Medium: {df_analysis[df_analysis['model']=='Medium']['word_count'].mean():.0f} avg words\")\n",
    "print(f\"   • Brutal: {df_analysis[df_analysis['model']=='Brutal']['word_count'].mean():.0f} avg words\")\n",
    "\n",
    "print(\"\\n4. USE CASES:\")\n",
    "print(\"   • Gentle: Best for sensitive job seekers, entry-level candidates\")\n",
    "print(\"   • Medium: Best for professionals seeking honest feedback\")\n",
    "print(\"   • Brutal: Best for entertainment, thick-skinned individuals\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'models': {\n",
    "        'gentle': {\n",
    "            'temperature': 0.4,\n",
    "            'num_critiques': len(gentle_results),\n",
    "            'avg_word_count': df_analysis[df_analysis['model']=='Gentle']['word_count'].mean(),\n",
    "        },\n",
    "        'medium': {\n",
    "            'temperature': 0.7,\n",
    "            'num_critiques': len(medium_results),\n",
    "            'avg_word_count': df_analysis[df_analysis['model']=='Medium']['word_count'].mean(),\n",
    "        },\n",
    "        'brutal': {\n",
    "            'temperature': 0.9,\n",
    "            'num_critiques': len(brutal_results),\n",
    "            'avg_word_count': df_analysis[df_analysis['model']=='Brutal']['word_count'].mean(),\n",
    "        }\n",
    "    },\n",
    "    'analysis': df_analysis.to_dict('records'),\n",
    "}\n",
    "\n",
    "if evaluations:\n",
    "    summary_report['evaluations'] = evaluations\n",
    "\n",
    "# Save report\n",
    "with open('../results/summary_report.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\" Summary report saved to results/summary_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This evaluation compared three CV roasting models with different temperatures:\n",
    "\n",
    "### Summary\n",
    "-  **Gentle Roaster (T=0.4)**: Consistent, professional and encouraging response\n",
    "-  **Medium Roaster (T=0.7)**: Direct, honest, yet balanced criticism\n",
    "-  **Brutal Roaster (T=0.9)**: Creative, humorous, outright savage roasts\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Temperature**: Higher temperatures produce (as suspected) more creative and varied outputs\n",
    "2. **Prompt engineering**: Different prompts create distinguishable different tones without fine-tuning\n",
    "3. **Trade-offs exist**: Between Consistency and creativity, professionalism and entertainment\n",
    "4. **Context matters**: Based on the user's needs and \"sensitivity\"\n",
    "\n",
    "### Future Improvements\n",
    "- Test with more diverse CVs\n",
    "- Add user feedback collection\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Next: 06_quick_cv_roaster.ipynb\n",
    "### Application with Input Options: \n",
    "1. **Drag & Drop**  PDF into the `uploaded_cvs/` folder\n",
    "2. **Use existing CV** from the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
