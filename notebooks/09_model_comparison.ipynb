{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Model Comparison: Base vs Fine-Tuned vs Gemini\n",
    "\n",
    "Compare three models on the same test CVs:\n",
    "1. **Base DistilGPT-2** (no fine-tuning)\n",
    "2. **Fine-Tuned DistilGPT-2** (after LoRA training)\n",
    "3. **Gemini 2.0 Flash** (reference baseline)\n",
    "\n",
    "## Evaluation Approach\n",
    "- Same 10 test CVs for all models\n",
    "- LLM-as-Judge evaluation\n",
    "- Statistical comparison\n",
    "- Memory-optimized for CPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:45:49.107792Z",
     "start_time": "2025-11-24T13:45:45.113446Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import google.generativeai as genai\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch & Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"✓ All imports loaded\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannokuegler/Library/CloudStorage/OneDrive-WUWien/SBWL/Data Science/4_LLM/roast_my_cv/roast_my_cv/.venv1/lib/python3.9/site-packages/google/api_core/_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.6) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/hannokuegler/Library/CloudStorage/OneDrive-WUWien/SBWL/Data Science/4_LLM/roast_my_cv/roast_my_cv/.venv1/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports loaded\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:45:49.228575Z",
     "start_time": "2025-11-24T13:45:49.224958Z"
    }
   },
   "source": [
    "# Load API key\n",
    "from config import GEMINI_API_KEY\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Paths\n",
    "MODEL_PATH = Path('../models/medium_roaster_lora')\n",
    "RESULTS_DIR = Path('../results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"✓ API configured\")\n",
    "print(f\"✓ Model path: {MODEL_PATH}\")\n",
    "print(f\"✓ Results directory: {RESULTS_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API configured\n",
      "✓ Model path: ../models/medium_roaster_lora\n",
      "✓ Results directory: ../results\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:45:49.328435Z",
     "start_time": "2025-11-24T13:45:49.237447Z"
    }
   },
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/resume_data.csv')\n",
    "\n",
    "# Load test indices\n",
    "with open('../data/test_cv_indices.json', 'r') as f:\n",
    "    test_cv_indices = json.load(f)['indices']\n",
    "\n",
    "print(f\"✓ Loaded {len(df):,} CVs\")\n",
    "print(f\"✓ Test set: {len(test_cv_indices)} CVs\")\n",
    "print(f\"   Indices: {test_cv_indices}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 9,544 CVs\n",
      "✓ Test set: 10 CVs\n",
      "   Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:45:49.347313Z",
     "start_time": "2025-11-24T13:45:49.341247Z"
    }
   },
   "source": [
    "# CV formatting function\n",
    "def format_cv_for_llm(resume_row):\n",
    "    \"\"\"Format a resume row into readable text.\"\"\"\n",
    "    cv_text = []\n",
    "    \n",
    "    if pd.notna(resume_row.get('career_objective')):\n",
    "        cv_text.append(f\"CAREER OBJECTIVE:\\n{resume_row['career_objective']}\")\n",
    "    \n",
    "    if pd.notna(resume_row.get('skills')):\n",
    "        cv_text.append(f\"\\nSKILLS:\\n{resume_row['skills']}\")\n",
    "    \n",
    "    education_parts = []\n",
    "    if pd.notna(resume_row.get('educational_institution_name')):\n",
    "        education_parts.append(f\"Institution: {resume_row['educational_institution_name']}\")\n",
    "    if pd.notna(resume_row.get('degree_names')):\n",
    "        education_parts.append(f\"Degree: {resume_row['degree_names']}\")\n",
    "    if pd.notna(resume_row.get('major_field_of_studies')):\n",
    "        education_parts.append(f\"Major: {resume_row['major_field_of_studies']}\")\n",
    "    if pd.notna(resume_row.get('passing_years')):\n",
    "        education_parts.append(f\"Year: {resume_row['passing_years']}\")\n",
    "    \n",
    "    if education_parts:\n",
    "        cv_text.append(f\"\\nEDUCATION:\\n\" + \"\\n\".join(education_parts))\n",
    "    \n",
    "    work_parts = []\n",
    "    if pd.notna(resume_row.get('professional_company_names')):\n",
    "        work_parts.append(f\"Company: {resume_row['professional_company_names']}\")\n",
    "    if pd.notna(resume_row.get('positions')):\n",
    "        work_parts.append(f\"Position: {resume_row['positions']}\")\n",
    "    if pd.notna(resume_row.get('start_dates')):\n",
    "        work_parts.append(f\"Period: {resume_row['start_dates']}\")\n",
    "        if pd.notna(resume_row.get('end_dates')):\n",
    "            work_parts[-1] += f\" to {resume_row['end_dates']}\"\n",
    "    if pd.notna(resume_row.get('responsibilities')):\n",
    "        work_parts.append(f\"Responsibilities:\\n{resume_row['responsibilities']}\")\n",
    "    \n",
    "    if work_parts:\n",
    "        cv_text.append(f\"\\nWORK EXPERIENCE:\\n\" + \"\\n\".join(work_parts))\n",
    "    \n",
    "    if pd.notna(resume_row.get('languages')):\n",
    "        cv_text.append(f\"\\nLANGUAGES:\\n{resume_row['languages']}\")\n",
    "    \n",
    "    if pd.notna(resume_row.get('certification_skills')):\n",
    "        cv_text.append(f\"\\nCERTIFICATIONS:\\n{resume_row['certification_skills']}\")\n",
    "    \n",
    "    return \"\\n\".join(cv_text)\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:45:51.151671Z",
     "start_time": "2025-11-24T13:45:49.355470Z"
    }
   },
   "source": [
    "print(\"Loading models...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"[1/3] Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"  ✓ Tokenizer loaded\")\n",
    "\n",
    "# Load base model\n",
    "print(\"\\n[2/3] Loading base DistilGPT-2...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "base_model.eval()\n",
    "print(f\"  ✓ Base model loaded ({base_model.num_parameters():,} parameters)\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "print(\"\\n[3/3] Loading fine-tuned model with LoRA...\")\n",
    "if MODEL_PATH.exists():\n",
    "    ft_base = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "    fine_tuned_model = PeftModel.from_pretrained(ft_base, MODEL_PATH)\n",
    "    fine_tuned_model.eval()\n",
    "    print(f\"  ✓ Fine-tuned model loaded from {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"  ✗ Fine-tuned model not found at {MODEL_PATH}\")\n",
    "    print(\"  → Run notebook 07 first to train the model\")\n",
    "    fine_tuned_model = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL LOADING COMPLETE\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "\n",
      "[1/3] Loading tokenizer...\n",
      "  ✓ Tokenizer loaded\n",
      "\n",
      "[2/3] Loading base DistilGPT-2...\n",
      "  ✓ Base model loaded (81,912,576 parameters)\n",
      "\n",
      "[3/3] Loading fine-tuned model with LoRA...\n",
      "  ✓ Fine-tuned model loaded from ../models/medium_roaster_lora\n",
      "\n",
      "================================================================================\n",
      "MODEL LOADING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:45:51.168864Z",
     "start_time": "2025-11-24T13:45:51.164646Z"
    }
   },
   "source": [
    "# Memory-optimized local model inference\n",
    "def generate_local_critique(model, cv_text, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate critique using local model (base or fine-tuned).\n",
    "    Optimized for low memory usage.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        return \"[Model not loaded]\"\n",
    "    \n",
    "    inputs_dict = None\n",
    "    outputs = None\n",
    "    \n",
    "    try:\n",
    "        # Short prompt to reduce memory\n",
    "        cv_short = cv_text[:400]\n",
    "        prompt = f\"Criticize this CV briefly:\\n{cv_short}\\n\\nCritique:\"\n",
    "        \n",
    "        # Tokenize with short max length\n",
    "        inputs_dict = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=200\n",
    "        )\n",
    "        \n",
    "        # Generate (greedy for consistency and less memory)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs_dict['input_ids'],\n",
    "                attention_mask=inputs_dict['attention_mask'],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # Greedy\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract critique\n",
    "        if \"Critique:\" in generated:\n",
    "            result = generated.split(\"Critique:\")[1].strip()\n",
    "        else:\n",
    "            result = generated\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"[Generation failed: {str(e)}]\"\n",
    "        \n",
    "    finally:\n",
    "        # Clean up\n",
    "        if inputs_dict is not None:\n",
    "            del inputs_dict\n",
    "        if outputs is not None:\n",
    "            del outputs\n",
    "        gc.collect()\n",
    "\n",
    "print(\"✓ Local model function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local model function defined\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T13:45:51.184726Z",
     "start_time": "2025-11-24T13:45:51.181620Z"
    }
   },
   "source": [
    "# Gemini inference\n",
    "ROASTER_PROMPT = \"\"\"You are an experienced hiring manager providing direct, honest CV feedback.\n",
    "\n",
    "Your approach:\n",
    "1. Be direct and honest - no sugarcoating\n",
    "2. Point out obvious flaws and red flags\n",
    "3. Call out generic buzzwords and filler content\n",
    "4. Be professional but don't hold back\n",
    "5. Focus on what actually matters to employers\n",
    "\n",
    "Structure:\n",
    "FIRST IMPRESSION: What stands out\n",
    "MAJOR ISSUES: Problems that need fixing\n",
    "CONCERNS: Things that raise questions\n",
    "WHAT WORKS: Brief strengths\n",
    "BOTTOM LINE: Final verdict\n",
    "\"\"\"\n",
    "\n",
    "def generate_gemini_critique(cv_text):\n",
    "    \"\"\"Generate critique using Gemini.\"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_output_tokens=1024,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    prompt = f\"{ROASTER_PROMPT}\\n\\nReview this CV:\\n\\n{cv_text}\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR: {str(e)}]\"\n",
    "\n",
    "print(\"✓ Gemini function defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini function defined\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Critiques from All Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-24T13:45:51.198711Z"
    }
   },
   "source": [
    "# Checkpoint file\n",
    "CHECKPOINT = Path('../data/model_comparison_checkpoint.json')\n",
    "\n",
    "# Load existing or start fresh\n",
    "if CHECKPOINT.exists():\n",
    "    with open(CHECKPOINT, 'r', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "    completed = [r['cv_idx'] for r in results]\n",
    "    print(f\"✓ Loaded {len(results)} completed evaluations\")\n",
    "else:\n",
    "    results = []\n",
    "    completed = []\n",
    "    print(\"Starting fresh...\")\n",
    "\n",
    "# Determine remaining\n",
    "remaining = [idx for idx in test_cv_indices if idx not in completed]\n",
    "\n",
    "print(f\"\\nProgress: {len(completed)}/{len(test_cv_indices)} CVs\")\n",
    "print(f\"Remaining: {remaining}\")\n",
    "print(f\"\\nEstimated time: ~{len(remaining) * 10} seconds\\n\")\n",
    "\n",
    "if len(remaining) > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"GENERATING CRITIQUES FROM ALL 3 MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for cv_idx in remaining:\n",
    "        print(f\"\\nCV #{cv_idx} ({test_cv_indices.index(cv_idx)+1}/{len(test_cv_indices)})\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Format CV\n",
    "            cv_text = format_cv_for_llm(df.iloc[cv_idx])\n",
    "            \n",
    "            result = {\n",
    "                'cv_idx': cv_idx,\n",
    "                'cv_text': cv_text\n",
    "            }\n",
    "            \n",
    "            # 1. Base model\n",
    "            print(\"  [1/3] Base model...\", end='', flush=True)\n",
    "            result['base_critique'] = generate_local_critique(base_model, cv_text, max_new_tokens=50)\n",
    "            print(\" ✓\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # 2. Fine-tuned model\n",
    "            print(\"  [2/3] Fine-tuned...\", end='', flush=True)\n",
    "            if fine_tuned_model is not None:\n",
    "                result['ft_critique'] = generate_local_critique(fine_tuned_model, cv_text, max_new_tokens=50)\n",
    "                print(\" ✓\")\n",
    "            else:\n",
    "                result['ft_critique'] = \"[Model not available]\"\n",
    "                print(\" ⊙ (skipped)\")\n",
    "            gc.collect()\n",
    "            \n",
    "            # 3. Gemini\n",
    "            print(\"  [3/3] Gemini...\", end='', flush=True)\n",
    "            result['gemini_critique'] = generate_gemini_critique(cv_text)\n",
    "            print(\" ✓\")\n",
    "            time.sleep(1.0)\n",
    "            \n",
    "            # Save result\n",
    "            results.append(result)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            with open(CHECKPOINT, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"  ✓ Saved ({len(results)}/{len(test_cv_indices)})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ✗ Error: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ COMPLETE: Generated {len(results)} x 3 = {len(results)*3} critiques\")\n",
    "print(f\"✓ Saved to: {CHECKPOINT}\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh...\n",
      "\n",
      "Progress: 0/10 CVs\n",
      "Remaining: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "Estimated time: ~100 seconds\n",
      "\n",
      "================================================================================\n",
      "GENERATING CRITIQUES FROM ALL 3 MODELS\n",
      "================================================================================\n",
      "\n",
      "CV #0 (1/10)\n",
      "------------------------------------------------------------\n",
      "  [1/3] Base model..."
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example outputs\n",
    "if len(results) > 0:\n",
    "    example = results[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE CRITIQUES - CV #0\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. BASE MODEL:\")\n",
    "    print(\"-\"*80)\n",
    "    print(example['base_critique'][:300])\n",
    "    \n",
    "    print(\"\\n2. FINE-TUNED MODEL:\")\n",
    "    print(\"-\"*80)\n",
    "    print(example['ft_critique'][:300])\n",
    "    \n",
    "    print(\"\\n3. GEMINI:\")\n",
    "    print(\"-\"*80)\n",
    "    print(example['gemini_critique'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLM-as-Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"You are an expert evaluator of CV critique quality.\n",
    "\n",
    "Evaluate this CV critique on these criteria (score 1-10 for each):\n",
    "\n",
    "1. **Specificity**: How specific and actionable is the feedback?\n",
    "2. **Relevance**: How relevant are the points to actual CV improvement?\n",
    "3. **Coherence**: Is the critique coherent and well-structured?\n",
    "4. **Completeness**: Does it cover important aspects of the CV?\n",
    "5. **Overall Usefulness**: How useful would this be to the job seeker?\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "  \"specificity\": <score>,\n",
    "  \"relevance\": <score>,\n",
    "  \"coherence\": <score>,\n",
    "  \"completeness\": <score>,\n",
    "  \"overall_usefulness\": <score>,\n",
    "  \"reasoning\": \"<brief explanation>\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_critique(critique_text, model_name, cv_text):\n",
    "    \"\"\"Use LLM to evaluate critique quality.\"\"\"\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        generation_config=genai.GenerationConfig(temperature=0.2)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"{JUDGE_PROMPT}\n",
    "\n",
    "Model: {model_name}\n",
    "\n",
    "Original CV (excerpt):\n",
    "{cv_text[:500]}...\n",
    "\n",
    "Critique to Evaluate:\n",
    "{critique_text}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        text = response.text\n",
    "        \n",
    "        # Extract JSON\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}') + 1\n",
    "        if start != -1 and end != 0:\n",
    "            json_str = text[start:end]\n",
    "            return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"✓ Judge function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATING ALL CRITIQUES WITH LLM JUDGE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nEvaluating {len(results)} CVs x 3 models = {len(results)*3} evaluations\")\n",
    "print(\"This will take ~3-5 minutes...\\n\")\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for result in tqdm(results, desc=\"Evaluating\"):\n",
    "    cv_idx = result['cv_idx']\n",
    "    cv_text = result['cv_text']\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name, critique_key in [\n",
    "        ('Base', 'base_critique'),\n",
    "        ('Fine-Tuned', 'ft_critique'),\n",
    "        ('Gemini', 'gemini_critique')\n",
    "    ]:\n",
    "        critique = result[critique_key]\n",
    "        \n",
    "        # Skip if error or not available\n",
    "        if critique.startswith('['):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            eval_result = evaluate_critique(critique, model_name, cv_text)\n",
    "            \n",
    "            if eval_result:\n",
    "                eval_result['model'] = model_name\n",
    "                eval_result['cv_idx'] = cv_idx\n",
    "                evaluations.append(eval_result)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on CV {cv_idx} - {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n✓ Completed {len(evaluations)} evaluations\")\n",
    "\n",
    "# Save evaluations\n",
    "eval_file = Path('../data/model_comparison_evaluations.json')\n",
    "with open(eval_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluations, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Saved to: {eval_file}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_eval = pd.DataFrame(evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "score_cols = ['specificity', 'relevance', 'coherence', 'completeness', 'overall_usefulness']\n",
    "df_eval['average_score'] = df_eval[score_cols].mean(axis=1)\n",
    "\n",
    "# Aggregate by model\n",
    "model_summary = df_eval.groupby('model')[score_cols + ['average_score']].agg(['mean', 'std']).round(2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTest CVs: {len(results)}\")\n",
    "print(f\"Evaluations per model: {df_eval.groupby('model').size().to_dict()}\")\n",
    "print(f\"\\nEvaluation method: LLM-as-Judge (Gemini 2.0 Flash)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON TABLE: Mean Scores (Scale 1-10)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Build comparison table\n",
    "comparison_data = []\n",
    "for model in ['Base', 'Fine-Tuned', 'Gemini']:\n",
    "    if model in model_summary.index:\n",
    "        row = {'Model': model}\n",
    "        for metric in score_cols:\n",
    "            mean_val = model_summary.loc[model, (metric, 'mean')]\n",
    "            std_val = model_summary.loc[model, (metric, 'std')]\n",
    "            row[metric.replace('_', ' ').title()] = f\"{mean_val:.2f} ± {std_val:.2f}\"\n",
    "        \n",
    "        mean_val = model_summary.loc[model, ('average_score', 'mean')]\n",
    "        std_val = model_summary.loc[model, ('average_score', 'std')]\n",
    "        row['Average'] = f\"{mean_val:.2f} ± {std_val:.2f}\"\n",
    "        \n",
    "        comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Overall averages\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL AVERAGE SCORES\")\n",
    "print(\"=\"*80)\n",
    "for model in ['Base', 'Fine-Tuned', 'Gemini']:\n",
    "    if model in model_summary.index:\n",
    "        mean_val = model_summary.loc[model, ('average_score', 'mean')]\n",
    "        std_val = model_summary.loc[model, ('average_score', 'std')]\n",
    "        print(f\"{model:15s}: {mean_val:.2f} ± {std_val:.2f} / 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SUMMARY BY MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\", df_eval.groupby('model')['average_score'].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Grouped bar chart\n",
    "ax1 = axes[0]\n",
    "metric_names = [m.replace('_', ' ').title() for m in score_cols]\n",
    "x = np.arange(len(metric_names))\n",
    "width = 0.25\n",
    "\n",
    "models_available = [m for m in ['Base', 'Fine-Tuned', 'Gemini'] if m in model_summary.index]\n",
    "colors = {'Base': '#d62728', 'Fine-Tuned': '#ff7f0e', 'Gemini': '#2ca02c'}\n",
    "\n",
    "for i, model in enumerate(models_available):\n",
    "    scores = [model_summary.loc[model, (m, 'mean')] for m in score_cols]\n",
    "    offset = (i - len(models_available)/2 + 0.5) * width\n",
    "    bars = ax1.bar(x + offset, scores, width, label=model, color=colors[model], alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Score (1-10)', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Evaluation Metrics', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Comparison Across All Metrics', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metric_names, rotation=45, ha='right')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim(0, 10)\n",
    "\n",
    "# Plot 2: Overall average with error bars\n",
    "ax2 = axes[1]\n",
    "means = [model_summary.loc[m, ('average_score', 'mean')] for m in models_available]\n",
    "stds = [model_summary.loc[m, ('average_score', 'std')] for m in models_available]\n",
    "bar_colors = [colors[m] for m in models_available]\n",
    "\n",
    "bars = ax2.bar(models_available, means, color=bar_colors, alpha=0.8, yerr=stds, capsize=10, error_kw={'linewidth': 2})\n",
    "\n",
    "ax2.set_ylabel('Average Score (1-10)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Overall Average Score (Mean ± Std)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, 10)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + std + 0.2,\n",
    "            f'{mean:.2f}±{std:.2f}', ha='center', va='bottom', \n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Figure saved: {RESULTS_DIR / 'model_comparison.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Prepare data\n",
    "heatmap_data = []\n",
    "for model in models_available:\n",
    "    row = [model_summary.loc[model, (metric, 'mean')] for metric in score_cols]\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "# Create heatmap\n",
    "im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=10)\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(np.arange(len(score_cols)))\n",
    "ax.set_yticks(np.arange(len(models_available)))\n",
    "ax.set_xticklabels([m.replace('_', ' ').title() for m in score_cols], rotation=45, ha='right')\n",
    "ax.set_yticklabels(models_available)\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Score (1-10)', rotation=270, labelpad=20, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(models_available)):\n",
    "    for j in range(len(score_cols)):\n",
    "        text = ax.text(j, i, f'{heatmap_data[i][j]:.1f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_title('Model Performance Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'model_comparison_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Heatmap saved: {RESULTS_DIR / 'model_comparison_heatmap.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export scores\n",
    "df_eval.to_csv(RESULTS_DIR / 'model_comparison_scores.csv', index=False)\n",
    "print(f\"✓ Scores saved: {RESULTS_DIR / 'model_comparison_scores.csv'}\")\n",
    "\n",
    "# Export summary\n",
    "summary_export = []\n",
    "for model in models_available:\n",
    "    for metric in score_cols + ['average_score']:\n",
    "        summary_export.append({\n",
    "            'Model': model,\n",
    "            'Metric': metric.replace('_', ' ').title(),\n",
    "            'Mean': model_summary.loc[model, (metric, 'mean')],\n",
    "            'Std': model_summary.loc[model, (metric, 'std')]\n",
    "        })\n",
    "\n",
    "pd.DataFrame(summary_export).to_csv(RESULTS_DIR / 'model_comparison_summary.csv', index=False)\n",
    "print(f\"✓ Summary saved: {RESULTS_DIR / 'model_comparison_summary.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. model_comparison.png - Bar charts\")\n",
    "print(\"  2. model_comparison_heatmap.png - Score heatmap\")\n",
    "print(\"  3. model_comparison_scores.csv - All evaluation scores\")\n",
    "print(\"  4. model_comparison_summary.csv - Summary statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate improvements\n",
    "if 'Base' in model_summary.index and 'Fine-Tuned' in model_summary.index:\n",
    "    base_avg = model_summary.loc['Base', ('average_score', 'mean')]\n",
    "    ft_avg = model_summary.loc['Fine-Tuned', ('average_score', 'mean')]\n",
    "    improvement = ft_avg - base_avg\n",
    "    improvement_pct = (improvement / base_avg) * 100\n",
    "    \n",
    "    print(f\"\\n1. Fine-Tuning Impact:\")\n",
    "    print(f\"   Base model:      {base_avg:.2f}/10\")\n",
    "    print(f\"   Fine-tuned:      {ft_avg:.2f}/10\")\n",
    "    print(f\"   Improvement:     {improvement:+.2f} points ({improvement_pct:+.1f}%)\")\n",
    "    \n",
    "    if improvement > 0.5:\n",
    "        print(\"   → Significant improvement from fine-tuning\")\n",
    "    elif improvement > 0:\n",
    "        print(\"   → Modest improvement from fine-tuning\")\n",
    "    else:\n",
    "        print(\"   → Minimal/no improvement from fine-tuning\")\n",
    "\n",
    "if 'Gemini' in model_summary.index:\n",
    "    gemini_avg = model_summary.loc['Gemini', ('average_score', 'mean')]\n",
    "    print(f\"\\n2. Gemini Performance:\")\n",
    "    print(f\"   Score: {gemini_avg:.2f}/10\")\n",
    "    \n",
    "    if 'Fine-Tuned' in model_summary.index:\n",
    "        gap = gemini_avg - ft_avg\n",
    "        print(f\"   Gap from fine-tuned: {gap:.2f} points\")\n",
    "        print(f\"   → Gemini is {gap:.1f}x better\" if gap > 0 else \"   → Models comparable\")\n",
    "\n",
    "# Best aspects\n",
    "print(f\"\\n3. Strongest Metrics Across All Models:\")\n",
    "overall_means = df_eval[score_cols].mean().sort_values(ascending=False)\n",
    "for i, (metric, score) in enumerate(overall_means.head(3).items(), 1):\n",
    "    print(f\"   {i}. {metric.replace('_', ' ').title()}: {score:.2f}/10\")\n",
    "\n",
    "print(f\"\\n4. Test Coverage:\")\n",
    "print(f\"   CVs evaluated: {len(results)}\")\n",
    "print(f\"   Total critiques: {len(results) * len(models_available)}\")\n",
    "print(f\"   Evaluation completeness: {len(evaluations)/(len(results)*len(models_available))*100:.0f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON COMPLETE ✓\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
